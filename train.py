#!/usr/bin/env python3
"""
æœ€ç»ˆä¼˜åŒ–è®­ç»ƒè„šæœ¬ - 1000å›åˆç‰ˆæœ¬
åŸºäºä¹‹å‰æµ‹è¯•çš„æœ€ä½³æ”¹è¿›ç­–ç•¥
"""
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import tankgame
import time
import math

class FinalOptimizedNet(nn.Module):
    """æœ€ç»ˆä¼˜åŒ–ç½‘ç»œæ¶æ„"""
    def __init__(self):
        super().__init__()
        
        # ä¸“æ³¨æ ¸å¿ƒç‰¹å¾çš„ç½‘ç»œç»“æ„
        self.shared_layers = nn.Sequential(
            nn.Linear(67, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 96),
            nn.ReLU()
        )
        
        # ç§»åŠ¨å¤´ - ç®€åŒ–ä½†æœ‰æ•ˆ
        self.movement_head = nn.Sequential(
            nn.Linear(96, 48),
            nn.ReLU(),
            nn.Linear(48, 5)
        )
        
        # ç„å‡†å¤´ - é‡ç‚¹ä¼˜åŒ–
        self.aim_head = nn.Sequential(
            nn.Linear(96, 64),
            nn.ReLU(),
            nn.Linear(64, 48),
            nn.ReLU(),
            nn.Linear(48, 3)
        )
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        shared_features = self.shared_layers(x)
        movement_q = self.movement_head(shared_features)
        aim_q = self.aim_head(shared_features)
        return movement_q, aim_q

class FinalOptimizedAgent:
    """æœ€ç»ˆä¼˜åŒ–æ™ºèƒ½ä½“"""
    def __init__(self):
        self.model = FinalOptimizedNet()
        self.target_model = FinalOptimizedNet()
        self.target_model.load_state_dict(self.model.state_dict())
        
        # ä¼˜åŒ–çš„ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=0.0005,
            weight_decay=1e-4
        )
        
        # ç¨³å®šçš„ç»éªŒå›æ”¾
        self.memory = deque(maxlen=15000)
        
        # ä¼˜åŒ–çš„æ¢ç´¢ç­–ç•¥
        self.epsilon = 0.6
        self.epsilon_decay = 0.997
        self.min_epsilon = 0.02
        
        # æ ¸å¿ƒè®­ç»ƒå‚æ•°
        self.gamma = 0.98
        self.batch_size = 64
        self.train_count = 0
        
        # è®­ç»ƒç»Ÿè®¡
        self.loss_history = []
        self.best_score = 0
        
    def remember(self, state, movement_action, aim_action, reward, next_state, done):
        self.memory.append((state, movement_action, aim_action, reward, next_state, done))
    
    def get_action(self, state, training=True):
        if training and random.random() < self.epsilon:
            # æ™ºèƒ½æ¢ç´¢ç­–ç•¥
            if random.random() < 0.7:
                movement_action = random.choices([0, 1, 2, 3, 4], weights=[0.1, 0.25, 0.25, 0.2, 0.2])[0]
                aim_action = random.choices([0, 1, 2], weights=[0.25, 0.25, 0.5])[0]
            else:
                movement_action = random.randint(0, 4)
                aim_action = random.randint(0, 2)
            return movement_action, aim_action
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            movement_q, aim_q = self.model(state_tensor)
            movement_action = movement_q.argmax().item()
            aim_action = aim_q.argmax().item()
        
        return movement_action, aim_action
    
    def train(self):
        if len(self.memory) < self.batch_size:
            return None
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.stack([torch.FloatTensor(b[0]) for b in batch])
        movement_actions = torch.LongTensor([b[1] for b in batch])
        aim_actions = torch.LongTensor([b[2] for b in batch])
        rewards = torch.FloatTensor([b[3] for b in batch])
        next_states = torch.stack([torch.FloatTensor(b[4]) for b in batch])
        dones = torch.BoolTensor([b[5] for b in batch])
        
        # å½“å‰Qå€¼
        current_movement_q, current_aim_q = self.model(states)
        current_movement_q = current_movement_q.gather(1, movement_actions.unsqueeze(1))
        current_aim_q = current_aim_q.gather(1, aim_actions.unsqueeze(1))
        
        # Double DQNç›®æ ‡Qå€¼
        with torch.no_grad():
            next_movement_q_online, next_aim_q_online = self.model(next_states)
            next_movement_actions = next_movement_q_online.max(1)[1]
            next_aim_actions = next_aim_q_online.max(1)[1]
            
            next_movement_q_target, next_aim_q_target = self.target_model(next_states)
            next_movement_q = next_movement_q_target.gather(1, next_movement_actions.unsqueeze(1)).squeeze()
            next_aim_q = next_aim_q_target.gather(1, next_aim_actions.unsqueeze(1)).squeeze()
        
        # ç›®æ ‡è®¡ç®—
        target_movement_q = rewards + (self.gamma * next_movement_q * ~dones)
        target_aim_q = rewards + (self.gamma * next_aim_q * ~dones)
        
        # æŸå¤±è®¡ç®— - é‡ç‚¹ä¼˜åŒ–ç„å‡†
        movement_loss = nn.SmoothL1Loss()(current_movement_q.squeeze(), target_movement_q)
        aim_loss = nn.SmoothL1Loss()(current_aim_q.squeeze(), target_aim_q)
        
        total_loss = movement_loss + 1.8 * aim_loss  # ç»™ç„å‡†æ›´é«˜æƒé‡
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        self.train_count += 1
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.train_count % 5 == 0:
            tau = 0.15
            for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):
                target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)
        
        return total_loss.item()
    
    def decay_epsilon(self):
        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)

def calculate_optimized_reward(game, last_score, last_lives, action_info):
    """ä¼˜åŒ–å¥–åŠ±å‡½æ•° - å¹³è¡¡ä¸”æœ‰æ•ˆ"""
    reward = 0
    
    # 1. æ ¸å¿ƒå‡»æ€å¥–åŠ± - æœ€é‡è¦
    score_delta = game.score - last_score
    if score_delta > 0:
        reward += score_delta * 30  # å¼ºå‡»æ€å¥–åŠ±
    
    # 2. ç”Ÿå­˜å¥–åŠ±
    if game.player.alive:
        reward += 0.5
    
    # 3. è¢«å‡»ä¸­æƒ©ç½š
    if game.player.lives < last_lives:
        reward -= 20
    
    # 4. æˆ˜æœ¯ä½ç½®å¥–åŠ±
    enemy = game._get_nearest_enemy()
    if enemy and enemy.alive:
        dist = tankgame.distance_between(game.player.x, game.player.y, enemy.x, enemy.y)
        
        # è·ç¦»å¥–åŠ±
        if 200 <= dist <= 350:
            reward += 2.0
        elif 150 <= dist < 200:
            reward += 1.0
        elif dist < 120:
            reward -= 0.8
        
        # ç²¾ç¡®ç„å‡†å¥–åŠ±
        dx = enemy.x - game.player.x
        dy = enemy.y - game.player.y
        target_angle = math.atan2(-dy, dx) % (2 * math.pi)
        angle_diff = abs(game.player.aim_angle - target_angle)
        angle_diff = min(angle_diff, 2*math.pi - angle_diff)
        
        if angle_diff < math.pi/45:  # 4åº¦å†…
            reward += 4.0
        elif angle_diff < math.pi/22:  # 8åº¦å†…
            reward += 2.5
        elif angle_diff < math.pi/12:  # 15åº¦å†…
            reward += 1.2
        
        # å°„å‡»å¥–åŠ± - ä»…åœ¨ç„å‡†è‰¯å¥½æ—¶
        if action_info['aim_action'] == 2:  # å°„å‡»åŠ¨ä½œ
            if angle_diff < math.pi/18:  # 10åº¦å†…å°„å‡»
                reward += 6.0
            elif angle_diff < math.pi/10:  # 18åº¦å†…
                reward += 3.0
            else:
                reward -= 1.5  # éšæ„å°„å‡»æƒ©ç½š
        
        # æ¥è¿‘æ•Œäººä½†ä¸å±é™©
        if 100 <= dist <= 200:
            reward += 1.2
    
    return reward

def train():
    """æœ€ç»ˆä¼˜åŒ–è®­ç»ƒ - 1000å›åˆ"""
    print("ğŸš€ æœ€ç»ˆä¼˜åŒ–è®­ç»ƒ (1000å›åˆ)")
    print("=" * 60)
    print("æ ¸å¿ƒæ”¹è¿›ï¼š")
    print("- ä¼˜åŒ–ç½‘ç»œæ¶æ„ (67â†’128â†’96)")
    print("- å¼ºåŒ–å‡»æ€å¥–åŠ± (30å€)")
    print("- æ™ºèƒ½æ¢ç´¢ç­–ç•¥")
    print("- ä¸“æ³¨ç„å‡†è®­ç»ƒ")
    print("- Double DQN + è½¯æ›´æ–°")
    print("- è‡ªé€‚åº”å­¦ä¹ ç‡")
    print("=" * 60)
    
    game = tankgame.TankGame(render=False)
    agent = FinalOptimizedAgent()
    
    scores = []
    game_scores = []
    start_time = time.time()
    
    # ä¿å­˜æ£€æŸ¥ç‚¹çš„å˜é‡
    best_model_score = 0
    last_save_time = time.time()
    
    for episode in range(1000):
        state = game.reset()
        total_reward = 0
        steps = 0
        episode_losses = []
        
        last_score = 0
        last_lives = game.player.lives
        
        while True:
            movement_action, aim_action = agent.get_action(state, training=True)
            action_info = {'aim_action': aim_action}
            
            # æ‰§è¡ŒåŠ¨ä½œ
            actions = []
            if movement_action == 1:
                actions.append(tankgame.ACTION_UP)
            elif movement_action == 2:
                actions.append(tankgame.ACTION_DOWN)
            elif movement_action == 3:
                actions.append(tankgame.ACTION_LEFT)
            elif movement_action == 4:
                actions.append(tankgame.ACTION_RIGHT)
            
            if aim_action == 0:
                actions.append(tankgame.ACTION_GUN_LEFT)
            elif aim_action == 1:
                actions.append(tankgame.ACTION_GUN_RIGHT)
            elif aim_action == 2:
                actions.append(tankgame.ACTION_SHOOT)
            
            game.do_actions(actions)
            reward, done = game.step()
            
            # è®¡ç®—ä¼˜åŒ–å¥–åŠ±
            optimized_reward = calculate_optimized_reward(game, last_score, last_lives, action_info)
            combined_reward = reward + optimized_reward
            
            last_score = game.score
            last_lives = game.player.lives
            
            next_state = game.get_state()
            
            agent.remember(state, movement_action, aim_action, combined_reward, next_state, done)
            
            # è®­ç»ƒ
            if steps % 1 == 0:
                loss = agent.train()
                if loss:
                    episode_losses.append(loss)
            
            state = next_state
            total_reward += combined_reward
            steps += 1
            
            if done or steps > 300:
                break
        
        scores.append(total_reward)
        game_scores.append(game.score)
        
        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon()
        
        # å®šæœŸä¿å­˜å’ŒæŠ¥å‘Š
        if episode % 50 == 0:
            avg_score = np.mean(scores[-50:]) if len(scores) >= 50 else np.mean(scores)
            avg_game_score = np.mean(game_scores[-50:]) if len(game_scores) >= 50 else np.mean(game_scores)
            avg_loss = np.mean(episode_losses) if episode_losses else 0
            elapsed_time = time.time() - start_time
            
            print(f"å›åˆ {episode:4d}: "
                  f"å¹³å‡å¥–åŠ±={avg_score:7.1f}, "
                  f"å½“å‰å¥–åŠ±={total_reward:7.1f}, "
                  f"å¹³å‡åˆ†={avg_game_score:5.1f}, "
                  f"æ¸¸æˆåˆ†={game.score:3d}, "
                  f"Îµ={agent.epsilon:.3f}, "
                  f"æŸå¤±={avg_loss:.1f}, "
                  f"ç”¨æ—¶={elapsed_time/60:.1f}min")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_game_score > best_model_score:
                best_model_score = avg_game_score
                torch.save(agent.model.state_dict(), "best_model.pth")
                print(f"          ğŸ† æ–°æœ€ä½³æ¨¡å‹! å¹³å‡åˆ†: {best_model_score:.1f}")
            
            # å®šæœŸå¤‡ä»½
            current_time = time.time()
            if current_time - last_save_time > 300:  # æ¯5åˆ†é’Ÿå¤‡ä»½ä¸€æ¬¡
                torch.save(agent.model.state_dict(), f"backup_model_ep{episode}.pth")
                last_save_time = current_time
                print(f"          ğŸ’¾ å¤‡ä»½æ¨¡å‹å·²ä¿å­˜: backup_model_ep{episode}.pth")
        
        # æ›´æ—©ä¸€äº›çš„é˜¶æ®µæ€§æŠ¥å‘Š
        elif episode % 10 == 0:
            avg_score = np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores)
            avg_game_score = np.mean(game_scores[-10:]) if len(game_scores) >= 10 else np.mean(game_scores)
            
            print(f"å›åˆ {episode:4d}: å¹³å‡å¥–åŠ±={avg_score:7.1f}, å¹³å‡åˆ†={avg_game_score:5.1f}, Îµ={agent.epsilon:.3f}")
    
    # æœ€ç»ˆç»“æœ
    total_time = time.time() - start_time
    final_avg_score = np.mean(scores[-50:])
    final_avg_game_score = np.mean(game_scores[-50:])
    
    print(f"\nğŸ† æœ€ç»ˆä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
    print(f"æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_score:.1f}")
    print(f"æœ€ç»ˆå¹³å‡æ¸¸æˆåˆ†æ•°: {final_avg_game_score:.1f}")
    print(f"æœ€é«˜æ¸¸æˆåˆ†æ•°: {max(game_scores):.1f}")
    print(f"è®­ç»ƒå›åˆæ•°: {len(scores)}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(agent.model.state_dict(), "final_model_1000.pth")
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ä¸º: final_model_1000.pth")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜ä¸º: best_model.pth (å¹³å‡åˆ†: {best_model_score:.1f})")
    
    return scores, game_scores

if __name__ == "__main__":
    scores, game_scores = train()