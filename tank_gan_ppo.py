
import pygame
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import math
import random
from collections import deque
import time
import os

# ============ åˆå§‹åŒ– ============
pygame.init()
pygame.font.init()

# ============ å¯¼å…¥æ¸¸æˆ ============
from tankgame import TankGame, ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, ACTION_GUN_LEFT, ACTION_GUN_RIGHT

# ============ ä¿®å¤ï¼šæ­£ç¡®çš„åŠ¨ä½œæ˜ å°„ ============
ACTION_MAP = {
    0: None,              # æ— åŠ¨ä½œ
    1: ACTION_UP,         # ä¸Š
    2: ACTION_DOWN,       # ä¸‹
    3: ACTION_LEFT,       # å·¦
    4: ACTION_RIGHT,      # å³
    5: ACTION_GUN_LEFT,   # ç‚®ç®¡å·¦è½¬
    6: ACTION_GUN_RIGHT   # ç‚®ç®¡å³è½¬
}

# åå‘æ˜ å°„ï¼šæ¸¸æˆåŠ¨ä½œ -> ç½‘ç»œåŠ¨ä½œç´¢å¼•
GAME_ACTION_TO_IDX = {v: k for k, v in ACTION_MAP.items() if v is not None}

# ============ ä¿®å¤ï¼šæ ¹æ®è¯Šæ–­ç»“æœè®¾ç½®æ­£ç¡®çš„çŠ¶æ€ç»´åº¦ ============
STATE_DIM = 12  # ä¿®æ­£ï¼šè¯Šæ–­ç»“æœæ˜¾ç¤ºçŠ¶æ€ç»´åº¦æ˜¯12
ACTION_DIM = 7  # 0-6ï¼Œä½†0æ˜¯æ— åŠ¨ä½œ

print(f"âœ… ä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€ç»´åº¦: {STATE_DIM}")

# ============ è®­ç»ƒå‚æ•° ============
LEARNING_RATE = 0.001
GAMMA = 0.99
BATCH_SIZE = 64
MEMORY_SIZE = 10000
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995

# ============ ç»éªŒå›æ”¾ç¼“å†²åŒº ============
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, min(len(self.buffer), batch_size))
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states), np.array(actions), np.array(rewards), 
                np.array(next_states), np.array(dones))
    
    def __len__(self):
        return len(self.buffer)

# ============ ä¿®å¤çš„ç¥ç»ç½‘ç»œï¼ˆè¾“å…¥ç»´åº¦12ï¼‰ ============
class SmartAIModel(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        
        print(f"ğŸ› ï¸ åˆ›å»ºç¥ç»ç½‘ç»œ: è¾“å…¥={state_dim}, è¾“å‡º={action_dim}")
        
        # æ ¹æ®12ç»´è¾“å…¥è°ƒæ•´ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, action_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for layer in self.net:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        return self.net(x)

# ============ DQN Agentï¼ˆä¿®å¤ç‰ˆï¼‰ ============
class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ¤– åˆ›å»ºæ™ºèƒ½ä½“: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}, è®¾å¤‡={self.device}")
        
        # ç½‘ç»œï¼ˆä½¿ç”¨æ­£ç¡®çš„è¾“å…¥ç»´åº¦ï¼‰
        self.policy_net = SmartAIModel(state_dim, action_dim).to(self.device)
        self.target_net = SmartAIModel(state_dim, action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)
        
        # ç»éªŒå›æ”¾
        self.memory = ReplayBuffer(MEMORY_SIZE)
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = EPSILON_START
        self.steps_done = 0
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_kills = []
        
        # è°ƒè¯•ä¿¡æ¯
        self.loss_history = []
    
    def select_action(self, state, game=None):
        """é€‰æ‹©åŠ¨ä½œï¼Œå¸¦æ¢ç´¢"""
        self.steps_done += 1
        
        # è¡°å‡epsilon
        self.epsilon = max(EPSILON_END, 
                          EPSILON_START * (EPSILON_DECAY ** self.steps_done))
        
        # epsilon-greedyç­–ç•¥
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œï¼Œä½†åå‘æœ‰ç”¨çš„åŠ¨ä½œ
            if random.random() < 0.6:  # 60%é€‰æ‹©ç„å‡†åŠ¨ä½œ
                return random.choice([5, 6])  # ç‚®ç®¡è½¬åŠ¨
            else:
                return random.choice([1, 2, 3, 4])  # ç§»åŠ¨
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            with torch.no_grad():
                # ç¡®ä¿çŠ¶æ€æ˜¯æ­£ç¡®çš„ç»´åº¦
                if len(state) != self.state_dim:
                    print(f"âš ï¸  çŠ¶æ€ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.state_dim}, å®é™…{len(state)}")
                    # æˆªæ–­æˆ–å¡«å……çŠ¶æ€
                    if len(state) > self.state_dim:
                        state = state[:self.state_dim]
                    else:
                        state = list(state) + [0] * (self.state_dim - len(state))
                
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.policy_net(state_tensor)
                
                # å¢å¼ºç„å‡†åŠ¨ä½œçš„Qå€¼ï¼ˆå¦‚æœæ¥è¿‘æ•Œäººï¼‰
                if game and hasattr(game, 'enemies') and game.enemies:
                    enemy = game.enemies[0]
                    dx = enemy.x - game.player.x
                    dy = enemy.y - game.player.y
                    target_angle = math.atan2(-dy, dx)
                    current_angle = game.player.aim_angle
                    
                    angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
                    angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
                    
                    # å¦‚æœæ²¡ç„å‡†å¥½ï¼Œå¢å¼ºç„å‡†åŠ¨ä½œ
                    if angle_diff > 0.3:  # å¤§äº17åº¦
                        q_values[0, 5] += 2.0  # ç‚®å·¦è½¬
                        q_values[0, 6] += 2.0  # ç‚®å³è½¬
                
                return q_values.argmax(dim=1).item()
    
    def optimize_model(self):
        """ä¼˜åŒ–æ¨¡å‹"""
        if len(self.memory) < BATCH_SIZE:
            return None
        
        try:
            # é‡‡æ ·
            states, actions, rewards, next_states, dones = self.memory.sample(BATCH_SIZE)
            
            # ç»´åº¦æ£€æŸ¥
            if states.shape[1] != self.state_dim:
                print(f"ğŸš¨ ä¸¥é‡é”™è¯¯: ç»éªŒæ± ä¸­çš„çŠ¶æ€ç»´åº¦ä¸åŒ¹é…")
                print(f"   æœŸæœ›: {self.state_dim}, å®é™…: {states.shape[1]}")
                # å°è¯•ä¿®å¤
                if states.shape[1] > self.state_dim:
                    states = states[:, :self.state_dim]
                    next_states = next_states[:, :self.state_dim]
                else:
                    padding = np.zeros((BATCH_SIZE, self.state_dim - states.shape[1]))
                    states = np.concatenate([states, padding], axis=1)
                    next_states = np.concatenate([next_states, padding], axis=1)
            
            # è½¬ä¸ºtensor
            states = torch.FloatTensor(states).to(self.device)
            actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)
            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
            next_states = torch.FloatTensor(next_states).to(self.device)
            dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)
            
            # è®¡ç®—å½“å‰Qå€¼
            current_q = self.policy_net(states).gather(1, actions)
            
            # è®¡ç®—ç›®æ ‡Qå€¼
            with torch.no_grad():
                next_q = self.target_net(next_states).max(1, keepdim=True)[0]
                target_q = rewards + (1 - dones) * GAMMA * next_q
            
            # è®¡ç®—æŸå¤±
            loss = nn.MSELoss()(current_q, target_q)
            
            # ä¼˜åŒ–
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
            
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            self.loss_history.append(loss.item())
            
            return loss.item()
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ¨¡å‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def update_target_net(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_net.load_state_dict(self.policy_net.state_dict())
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'steps_done': self.steps_done,
            'episode_rewards': self.episode_rewards,
            'episode_kills': self.episode_kills,
            'loss_history': self.loss_history
        }, path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return False
            
        try:
            checkpoint = torch.load(path)
            self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
            self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.epsilon = checkpoint['epsilon']
            self.steps_done = checkpoint['steps_done']
            self.episode_rewards = checkpoint['episode_rewards']
            self.episode_kills = checkpoint['episode_kills']
            self.loss_history = checkpoint.get('loss_history', [])
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {path}")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

# ============ å¢å¼ºçš„å¥–åŠ±å‡½æ•° ============
def calculate_reward(game, prev_score, prev_enemies_count):
    """è®¡ç®—å¢å¼ºçš„å¥–åŠ±"""
    reward = 0.0
    
    # 1. å‡»æ€å¥–åŠ±ï¼ˆæœ€é‡è¦ï¼‰
    current_score = game.score
    if current_score > prev_score:
        kill_reward = 100.0  # å¤§å¹…å¥–åŠ±å‡»æ€
        reward += kill_reward
        print(f"ğŸ¯ å‡»æ€å¥–åŠ±: +{kill_reward}")
    
    # 2. ç”Ÿå­˜å¥–åŠ±
    reward += 0.1  # æ¯æ­¥ç”Ÿå­˜å¥–åŠ±
    
    # 3. ç„å‡†è´¨é‡å¥–åŠ±
    if game.enemies:
        enemy = game.enemies[0]
        dx = enemy.x - game.player.x
        dy = enemy.y - game.player.y
        target_angle = math.atan2(-dy, dx)
        current_angle = game.player.aim_angle
        
        angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
        angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
        
        # ç„å‡†è¶Šå¥½å¥–åŠ±è¶Šé«˜
        aim_reward = 0.5 * (1.0 - angle_diff / math.pi)
        reward += aim_reward
    
    # 4. æƒ©ç½šæ— æ•ˆå¼€ç«
    if game.player.auto_shoot:
        reward -= 0.01  # è½»å¾®æƒ©ç½šå¼€ç«æ¶ˆè€—
    
    # 5. æ¥è¿‘æ•Œäººå¥–åŠ±
    if game.enemies:
        enemy = game.enemies[0]
        distance = math.sqrt((enemy.x - game.player.x)**2 + (enemy.y - game.player.y)**2)
        max_distance = math.sqrt(800**2 + 600**2)  # å‡è®¾å±å¹•å¤§å°
        distance_reward = 0.1 * (1.0 - distance / max_distance)
        reward += distance_reward
    
    return reward

# ============ è®­ç»ƒå¾ªç¯ï¼ˆä¿®å¤ç‰ˆï¼‰ ============
def train_dqn():
    print("ğŸš€ å¼€å§‹DQNè®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºæ¸¸æˆ
    game = TankGame(render=False)
    
    # æœ€ç»ˆç¡®è®¤çŠ¶æ€ç»´åº¦
    test_state = game.get_state()
    actual_state_dim = len(test_state)
    print(f"âœ… æœ€ç»ˆç¡®è®¤çŠ¶æ€ç»´åº¦: {actual_state_dim}")
    print(f"   æ ·æœ¬çŠ¶æ€: {test_state[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ªå€¼
    
    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€ç»´åº¦
    global STATE_DIM
    if STATE_DIM != actual_state_dim:
        print(f"âš ï¸  ä¿®æ­£STATE_DIM: {STATE_DIM} -> {actual_state_dim}")
        STATE_DIM = actual_state_dim
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = DQNAgent(STATE_DIM, ACTION_DIM)
    
    # è®­ç»ƒå‚æ•°
    num_episodes = 2000
    target_update = 10  # æ¯10è½®æ›´æ–°ç›®æ ‡ç½‘ç»œ
    save_interval = 100  # æ¯100è½®ä¿å­˜æ¨¡å‹
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs("./checkpoints", exist_ok=True)
    
    # é¢„å¡«å……ç»éªŒæ± 
    print("\nğŸ“¦ é¢„å¡«å……ç»éªŒæ± ...")
    prefill_steps = 0
    while len(agent.memory) < BATCH_SIZE * 2:
        state = game.reset()
        prev_score = game.score
        
        for step in range(50):
            # éšæœºåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
            action = random.choice([1, 2, 3, 4, 5, 6])
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if action in ACTION_MAP and ACTION_MAP[action]:
                game.do_action(ACTION_MAP[action])
            
            # è‡ªåŠ¨å¼€ç«é€»è¾‘
            if action in [5, 6] and game.enemies:
                enemy = game.enemies[0]
                dx = enemy.x - game.player.x
                dy = enemy.y - game.player.y
                target_angle = math.atan2(-dy, dx)
                current_angle = game.player.aim_angle
                
                angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
                angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
                
                if angle_diff / math.pi < 0.3:
                    game.player.auto_shoot = True
            
            # æ¸¸æˆæ­¥è¿›
            game.step()
            next_state = game.get_state()
            
            # è®¡ç®—å¥–åŠ±
            reward = calculate_reward(game, prev_score, 0)
            prev_score = game.score
            
            # å­˜å‚¨ç»éªŒ
            done = game.game_over
            agent.memory.push(state, action, reward, next_state, done)
            
            state = next_state
            prefill_steps += 1
            
            if done:
                break
            
            if prefill_steps >= 1000:  # é˜²æ­¢æ— é™å¾ªç¯
                break
    
    print(f"âœ… ç»éªŒæ± é¢„å¡«å……å®Œæˆ: {len(agent.memory)} æ¡ç»éªŒ, {prefill_steps} æ­¥")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    print("\nğŸ® å¼€å§‹ä¸»è®­ç»ƒå¾ªç¯...")
    print("=" * 60)
    
    for episode in range(num_episodes):
        # é‡ç½®ç¯å¢ƒ
        state = game.reset()
        prev_score = game.score
        episode_reward = 0
        episode_kills = 0
        
        # è·å–åˆå§‹å‡»æ€æ•°
        initial_kills = game.score // 70 if hasattr(game, 'score') else 0
        
        for step in range(300):  # æ¯å›åˆæœ€å¤š300æ­¥
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, game)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if action in ACTION_MAP and ACTION_MAP[action]:
                game.do_action(ACTION_MAP[action])
            
            # æ™ºèƒ½è‡ªåŠ¨å¼€ç«
            if game.enemies and action in [5, 6]:
                enemy = game.enemies[0]
                dx = enemy.x - game.player.x
                dy = enemy.y - game.player.y
                target_angle = math.atan2(-dy, dx)
                current_angle = game.player.aim_angle
                
                angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
                angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
                
                # åªæœ‰ç„å‡†å¾—æ¯”è¾ƒå¥½æ—¶æ‰å¼€ç«
                if angle_diff / math.pi < 0.2:  # 20%è¯¯å·®å†…
                    game.player.auto_shoot = True
                else:
                    game.player.auto_shoot = False
            else:
                game.player.auto_shoot = False
            
            # æ¸¸æˆæ­¥è¿›
            game.step()
            next_state = game.get_state()
            
            # è®¡ç®—å¥–åŠ±
            reward = calculate_reward(game, prev_score, 0)
            prev_score = game.score
            
            # ç»Ÿè®¡å‡»æ€
            current_kills = game.score // 70 if hasattr(game, 'score') else 0
            if current_kills > episode_kills:
                episode_kills = current_kills
                print(f"ğŸ¯ å›åˆ{episode+1} æ­¥{step}: å‡»æ€! æ€»å‡»æ€{episode_kills}")
            
            # å­˜å‚¨ç»éªŒ
            done = game.game_over or step == 299
            agent.memory.push(state, action, reward, next_state, done)
            
            # ä¼˜åŒ–æ¨¡å‹
            loss = agent.optimize_model()
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward
            
            if done:
                break
        
        # è®°å½•ç»Ÿè®¡
        agent.episode_rewards.append(episode_reward)
        agent.episode_kills.append(episode_kills)
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % target_update == 0:
            agent.update_target_net()
        
        # æ‰“å°è¿›åº¦
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(agent.episode_rewards[-10:]) if len(agent.episode_rewards) >= 10 else episode_reward
            avg_kills = np.mean(agent.episode_kills[-10:]) if len(agent.episode_kills) >= 10 else episode_kills
            
            print(f"å›åˆ {episode+1:4d} | "
                  f"å¥–åŠ±: {episode_reward:6.1f} | "
                  f"å‡»æ€: {episode_kills:2d} | "
                  f"Epsilon: {agent.epsilon:.3f} | "
                  f"ç»éªŒæ± : {len(agent.memory):5d}")
        
        # ä¿å­˜æ¨¡å‹
        if (episode + 1) % save_interval == 0:
            agent.save_model(f"./checkpoints/model_episode_{episode+1}.pth")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save_model("./tank_ai_final.pth")
    print("\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸º tank_ai_final.pth")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if len(agent.episode_rewards) > 0:
        try:
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(12, 4))
            
            # å¥–åŠ±æ›²çº¿
            plt.subplot(1, 3, 1)
            plt.plot(agent.episode_rewards)
            plt.title('Episode Rewards')
            plt.xlabel('Episode')
            plt.ylabel('Reward')
            
            # å‡»æ€æ›²çº¿
            plt.subplot(1, 3, 2)
            plt.plot(agent.episode_kills)
            plt.title('Episode Kills')
            plt.xlabel('Episode')
            plt.ylabel('Kills')
            
            # æŸå¤±æ›²çº¿
            plt.subplot(1, 3, 3)
            if agent.loss_history:
                # å¹³æ»‘æŸå¤±
                window = 50
                smoothed_loss = []
                for i in range(len(agent.loss_history)):
                    start = max(0, i - window)
                    smoothed_loss.append(np.mean(agent.loss_history[start:i+1]))
                plt.plot(smoothed_loss)
                plt.title('Training Loss (Smoothed)')
                plt.xlabel('Optimization Step')
                plt.ylabel('Loss')
            
            plt.tight_layout()
            plt.savefig('./training_history.png')
            plt.show()
            print("ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º training_history.png")
        except:
            print("âš ï¸  æ— æ³•ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆå¯èƒ½éœ€è¦å®‰è£…matplotlibï¼‰")
    
    pygame.quit()
    return agent

# ============ æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ ============
def test_trained_model(model_path=None):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("\nğŸ§ª æµ‹è¯•AIæ€§èƒ½")
    print("=" * 60)
    
    # ç¡®ä¿Pygameåˆå§‹åŒ–
    try:
        pygame.init()
        pygame.font.init()
    except:
        pass
    
    game = TankGame(render=True)
    
    # è·å–çŠ¶æ€ç»´åº¦
    test_state = game.get_state()
    state_dim = len(test_state)
    print(f"æµ‹è¯•çŠ¶æ€ç»´åº¦: {state_dim}")
    
    agent = DQNAgent(state_dim, ACTION_DIM)
    
    if model_path and os.path.exists(model_path):
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        if not agent.load_model(model_path):
            print("ä½¿ç”¨æ–°æ¨¡å‹")
    else:
        print("ä½¿ç”¨æ–°æ¨¡å‹")
    
    agent.epsilon = 0.01  # æµ‹è¯•æ—¶ç”¨å¾ˆå°çš„æ¢ç´¢ç‡
    
    num_test_episodes = 5
    total_kills = 0
    total_steps = 0
    
    for episode in range(num_test_episodes):
        state = game.reset()
        episode_kills = 0
        episode_steps = 0
        
        print(f"\næµ‹è¯•å›åˆ {episode+1}:")
        
        for step in range(200):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, game)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if action in ACTION_MAP and ACTION_MAP[action]:
                game.do_action(ACTION_MAP[action])
            
            # è‡ªåŠ¨å¼€ç«
            if action in [5, 6]:
                game.player.auto_shoot = True
            
            # æ¸¸æˆæ­¥è¿›
            game.step()
            next_state = game.get_state()
            
            # æ£€æŸ¥å‡»æ€
            kills = game.score // 70 if hasattr(game, 'score') else 0
            if kills > episode_kills:
                episode_kills = kills
                print(f"  æ­¥{step}: å‡»æ€ï¼æ€»å‡»æ€{kills}")
            
            # æ›´æ–°
            state = next_state
            episode_steps += 1
            
            if game.game_over:
                break
            
            # å¤„ç†é€€å‡ºäº‹ä»¶
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    return
        
        total_kills += episode_kills
        total_steps += episode_steps
        
        print(f"  ç»“æŸ: å‡»æ€={episode_kills}, æ­¥æ•°={episode_steps}")
    
    pygame.quit()
    
    avg_kills = total_kills / num_test_episodes
    avg_steps = total_steps / num_test_episodes
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  å¹³å‡æ¯å›åˆå‡»æ€: {avg_kills:.2f}")
    print(f"  å¹³å‡æ¯å›åˆæ­¥æ•°: {avg_steps:.2f}")
    
    if avg_kills > 0.5:
        print("âœ… AIå­¦ä¹ æˆåŠŸï¼")
    else:
        print("âš ï¸  AIä»éœ€æ”¹è¿›")

# ============ ä¸»å‡½æ•° ============
def main():
    print("ğŸ¯ å¦å…‹æ¸¸æˆAI - ä¿®å¤ç‰ˆè®­ç»ƒè„šæœ¬")
    print("=" * 60)
    print("ä¸»è¦ä¿®å¤:")
    print("1. âœ… çŠ¶æ€ç»´åº¦ä»14ä¿®æ­£ä¸º12")
    print("2. âœ… ç¥ç»ç½‘ç»œè¾“å…¥å±‚åŒ¹é…å®é™…çŠ¶æ€ç»´åº¦")
    print("3. âœ… æ·»åŠ ç»´åº¦æ£€æŸ¥å’Œä¿®å¤æœºåˆ¶")
    print("4. âœ… æ”¹è¿›å¥–åŠ±å‡½æ•°")
    print("5. âœ… æ›´å¥½çš„è®­ç»ƒç›‘æ§")
    print("=" * 60)
    
    while True:
        print("\né€‰é¡¹:")
        print("1. å¼€å§‹è®­ç»ƒDQN")
        print("2. æµ‹è¯•ç°æœ‰æ¨¡å‹")
        print("3. è¿è¡Œå¿«é€Ÿè¯Šæ–­")
        print("4. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        if choice == "1":
            print("\nå¼€å§‹è®­ç»ƒ...")
            agent = train_dqn()
            
            # è®­ç»ƒåç«‹å³æµ‹è¯•
            test_trained_model("./tank_ai_final.pth")
            
        elif choice == "2":
            model_path = input("æ¨¡å‹è·¯å¾„ (é»˜è®¤: ./tank_ai_final.pth): ").strip()
            if not model_path:
                model_path = "./tank_ai_final.pth"
            test_trained_model(model_path)
            
        elif choice == "3":
            print("\nè¿è¡Œå¿«é€Ÿè¯Šæ–­...")
            from tankgame import TankGame
            
            game = TankGame(render=False)
            
            # çŠ¶æ€ç»´åº¦è¯Šæ–­
            state = game.get_state()
            print(f"çŠ¶æ€ç»´åº¦: {len(state)}")
            print(f"çŠ¶æ€å†…å®¹: {state}")
            
            # æµ‹è¯•åŠ¨ä½œæ‰§è¡Œ
            print("\næµ‹è¯•åŠ¨ä½œæ‰§è¡Œ:")
            for action_name, action in ACTION_MAP.items():
                if action:
                    print(f"  åŠ¨ä½œ{action_name}: {action}")
            
            # æµ‹è¯•å‡ è½®éšæœºAI
            print("\næµ‹è¯•éšæœºAIæ€§èƒ½:")
            total_kills = 0
            for ep in range(3):
                game.reset()
                kills = 0
                for step in range(100):
                    action = random.choice([1, 2, 3, 4, 5, 6])
                    if action in ACTION_MAP and ACTION_MAP[action]:
                        game.do_action(ACTION_MAP[action])
                    
                    if random.random() < 0.3:
                        game.player.auto_shoot = True
                    
                    game.step()
                    
                    current_kills = game.score // 70
                    if current_kills > kills:
                        kills = current_kills
                
                total_kills += kills
                print(f"  å›åˆ{ep+1}: å‡»æ€{kills}")
            
            print(f"éšæœºAIå¹³å‡å‡»æ€: {total_kills/3:.1f}")
            
        elif choice == "4":
            print("ğŸ‘‹ é€€å‡º")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import torch
        main()
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install torch numpy")