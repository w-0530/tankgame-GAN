import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pygame
import math
import random
from tqdm import tqdm
from collections import deque

# å¯¼å…¥æ¸¸æˆæ ¸å¿ƒç±»+å¸¸é‡+åŠ¨ä½œï¼ˆä¿®å¤WALL_SIZEæœªå®šä¹‰ï¼‰
from tankgame import (
    TankGame, ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT,
    ACTION_GUN_LEFT, ACTION_GUN_RIGHT, WALL_SIZE
)

# ====================== åŸºç¡€é…ç½® =======================
os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = '1'
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"âœ… è®­ç»ƒè®¾å¤‡ï¼š{DEVICE} | CUDAå¯ç”¨ï¼š{torch.cuda.is_available()}")

# æ ¸å¿ƒç»´åº¦ï¼ˆAIä»…è¾“å‡º0/1åŠ¨ä½œç±»å‹ï¼Œæ˜ å°„æ¸¸æˆå®é™…åŠ¨ä½œï¼‰
STATE_DIM = 14
ACTION_DIM = 2  # 0=ç§»åŠ¨ 1=ç„å‡†
DEMO_VEC_DIM = STATE_DIM + ACTION_DIM

# ====================== è¶…å‚æ•°ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šå¼ºåŒ–ç„å‡†ã€å¼±åŒ–ç§»åŠ¨ï¼‰ ======================
# PPOè¶…å‚æ•°
PPO_LR = 3e-4
GAMMA = 0.98
LAMBDA = 0.85
EPS_CLIP = 0.25
BATCH_SIZE = 32
ENT_COEF = 0.05  # é™ä½æ¢ç´¢ç³»æ•°ï¼Œé¿å…æ— æ„ä¹‰ä¹±åŠ¨
MAX_STEP = 350
# GANè¶…å‚æ•°ï¼ˆä¿å®ˆæƒé‡ï¼Œä¸ä¸»å¯¼è®­ç»ƒï¼‰
GAN_LR = 4e-5
GAN_UPDATE_INTERVAL = 5
GAN_REWARD_WEIGHT = 0.05  # è¿›ä¸€æ­¥é™ä½GANæƒé‡

# ====================== å¥–åŠ±/æƒ©ç½šé…ç½®ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç„å‡†é›¶é—¨æ§›+æ¢¯åº¦åŒ– | å‘æ•Œå¥–åŠ±ï¼‰ ======================
REWARD_DIRECT_SHOT_POS = 1.5     # é™ä½ç§»åŠ¨å¥–åŠ±ï¼Œé¿å…ç§»åŠ¨ä¸»å¯¼
REWARD_DODGE_BULLET = 0.8        # é™ä½èº²å­å¼¹å¥–åŠ±ï¼Œæ¶ˆæé˜²å¾¡ä¸ä¼˜å…ˆ
REWARD_AIM_GRADIENT = 3.0        # é›¶é—¨æ§›æ¢¯åº¦ç„å‡†å¥–åŠ±ï¼ˆåªè¦è½¬ç‚®ç®¡å°±ç»™ï¼‰
REWARD_AIM_GOOD = 4.0            # è¾ƒå¥½ç„å‡†å¥–åŠ±ï¼ˆè¯¯å·®<0.3ï¼Œæ˜“è§¦å‘ï¼‰
REWARD_AIM_PERFECT = 6.0         # å®Œç¾ç„å‡†å¥–åŠ±ï¼ˆé—¨æ§›é™ä½åˆ°0.2ï¼‰
REWARD_TOWARD_ENEMY = 1.2        # æ ¸å¿ƒï¼šå‘æ•Œäººç§»åŠ¨å¥–åŠ±ï¼Œå¼•å¯¼æœ‰æ„è¯†è·‘ä½
PUNISH_NO_KILL = 0.8             # é™ä½æœªå‡»æ€æƒ©ç½šï¼Œç»™AIç„å‡†æ—¶é—´
PUNISH_IDLE = 0.3                # å¤§å¹…é™ä½é‡å¤åŠ¨ä½œæƒ©ç½šï¼Œå…è®¸è¿ç»­ç„å‡†
PUNISH_BEEN_HIT = 20.0           # é™ä½è¢«å‡»ä¸­æƒ©ç½šï¼Œè®­ç»ƒåˆæœŸä¸æ€•å¤±è¯¯
# åˆ¤å®šé˜ˆå€¼ï¼ˆå…¨é‡é™ä½é—¨æ§›ï¼Œè®­ç»ƒåˆæœŸæ˜“è§¦å‘ï¼‰
BULLET_DODGE_DIST = 60           # æé«˜èº²å­å¼¹å®‰å…¨è·ç¦»
NO_KILL_STEP_THRESH = 30         # æé«˜æœªå‡»æ€æƒ©ç½šæ­¥æ•°
CLOSE_DIST_THRESH = 250          # æ‰©å¤§æ•Œäººæš´éœ²è·ç¦»
RAYCAST_STEP = 15                # é™ä½å°„çº¿æ£€æµ‹ç²¾åº¦ï¼Œç›´å°„ä½æ˜“åˆ¤å®š
AIM_GOOD_THRESH = 0.3            # è¾ƒå¥½ç„å‡†é˜ˆå€¼ï¼ˆ30%è¯¯å·®ï¼‰
AIM_PERFECT_THRESH = 0.2         # å®Œç¾ç„å‡†é˜ˆå€¼ï¼ˆ20%è¯¯å·®ï¼Œæ˜“è§¦å‘ï¼‰

# ====================== è®­ç»ƒé…ç½®ï¼ˆå¿«é€ŸéªŒè¯ï¼Œ100è½®è§æ•ˆæœï¼‰ ======================
MEMORY_CAPACITY = 40000  # å‡å°ç»éªŒæ± ï¼Œè®­ç»ƒæ›´æ–°æ›´å¿«
DEMO_MEMORY_CAPACITY = 3000
TRAIN_EPISODES = 1200     # å‡å°‘è®­ç»ƒè½®æ•°ï¼Œå¿«é€ŸéªŒè¯
SAVE_INTERVAL = 50       # æé«˜ä¿å­˜é¢‘ç‡ï¼Œæ¯50è½®æµ‹è¯•ä¸€æ¬¡
RENDER_TRAIN = False
RENDER_TEST = True

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs("./tank_ai_models_simple", exist_ok=True)
os.makedirs("./tank_demo_data_simple", exist_ok=True)

# =========================================================
# PPO ç»éªŒç¼“å†²åŒº
# =========================================================
class PPOMemory:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = deque(maxlen=capacity)
    
    def add(self, s, a, r, ns, d, p):
        self.memory.append((s, a, r, ns, d, p))
    
    def sample(self, batch_size):
        if len(self.memory) < batch_size:
            return None
        idx = np.random.choice(len(self.memory), batch_size, replace=False)
        data = [self.memory[i] for i in idx]
        s = torch.FloatTensor([d[0] for d in data]).to(DEVICE)
        a = torch.LongTensor([d[1] for d in data]).unsqueeze(1).to(DEVICE)
        r = torch.FloatTensor([d[2] for d in data]).unsqueeze(1).to(DEVICE)
        ns = torch.FloatTensor([d[3] for d in data]).to(DEVICE)
        d = torch.FloatTensor([d[4] for d in data]).unsqueeze(1).to(DEVICE)
        p = torch.FloatTensor([d[5] for d in data]).unsqueeze(1).to(DEVICE)
        return s, a, r, ns, d, p
    
    def __len__(self):
        return len(self.memory)

# =========================================================
# ä¸“å®¶ç»éªŒç¼“å†²åŒº
# =========================================================
class DemoMemory:
    def __init__(self, capacity, vec_dim):
        self.capacity = capacity
        self.vec_dim = vec_dim
        self.memory = deque(maxlen=capacity)
    
    def add(self, demo_vec):
        assert demo_vec.shape[0] == self.vec_dim, f"ç»´åº¦é”™è¯¯ï¼šå®é™…{demo_vec.shape[0]}ï¼ŒæœŸæœ›{self.vec_dim}"
        self.memory.append(demo_vec)
    
    def sample(self, batch_size):
        batch_size = min(batch_size, len(self.memory))
        idx = np.random.choice(len(self.memory), batch_size, replace=False)
        demo_array = np.array([self.memory[i] for i in idx], dtype=np.float32)
        return torch.FloatTensor(demo_array).to(DEVICE)
    
    def save_to_npy(self, npy_path):
        np.save(npy_path, np.array(self.memory, dtype=np.float32))
        print(f"ğŸ’¾ ä¸“å®¶ç»éªŒä¿å­˜è‡³ï¼š{npy_path}")
    
    def load_from_npy(self, npy_path):
        if os.path.exists(npy_path):
            demo_array = np.load(npy_path)
            for vec in demo_array:
                self.add(vec)
            print(f"ğŸ“š åŠ è½½ä¸“å®¶ç»éªŒï¼š{len(demo_array)}æ¡ | ç»´åº¦ï¼š{demo_array.shape[1]}")
    
    def __len__(self):
        return len(self.memory)

# =========================================================
# PPO ç½‘ç»œï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šLeakyReLUâ†’ReLUï¼Œé€‚é…æµ…å±‚æ¨¡å‹+åˆå§‹åŒ–ä¼˜åŒ–ï¼‰
# =========================================================
class PPO_Actor(nn.Module):
    def __init__(self, state_dim, action_dim=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),  # æ›¿æ¢LeakyReLUï¼Œæ¢¯åº¦æ›´ç›´æ¥
            nn.Linear(64, 32),
            nn.ReLU(),  # æ›¿æ¢LeakyReLU
            nn.Linear(32, action_dim)
        ).to(DEVICE)
        self.MOVE_ACTION_LIST = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]
        self.AIM_ACTION_LIST = [ACTION_GUN_LEFT, ACTION_GUN_RIGHT]
        # æ ¸å¿ƒï¼šç„å‡†åŠ¨ä½œæ¢ç´¢åå‘ï¼Œå¼ºåˆ¶AIæ¢ç´¢ç„å‡†ï¼ˆæ‰“ç ´ç§»åŠ¨æ­»å¾ªç¯ï¼‰
        self.aim_explore_bias = 0.4
        # æµ…å±‚ç½‘ç»œåˆå§‹åŒ–ä¼˜åŒ–ï¼šåç½®ç½®0ï¼Œé¿å…åˆå§‹å€¼å¹²æ‰°
        for m in self.net.modules():
            if isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0.0)

    def forward(self, x):
        logits = self.net(x.to(DEVICE))
        # ç»™ç„å‡†åŠ¨ä½œï¼ˆç´¢å¼•1ï¼‰åŠ åå‘ï¼Œæå‡å…¶è¢«é€‰ä¸­çš„æ¦‚ç‡
        logits[:, 1] += self.aim_explore_bias
        return F.softmax(logits, dim=-1)

    def get_action(self, state):
        """è®­ç»ƒç”¨ï¼šè¿”å›æ¸¸æˆåŠ¨ä½œã€AI0/1åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡ | è®­ç»ƒæœŸä¹ŸåŠ ç„å‡†æ–¹å‘åå¥½"""
        s = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            probs = self(s)
        ai_action = torch.multinomial(probs, 1).item()
        # è®­ç»ƒæœŸç„å‡†æ–¹å‘6:4åå¥½ï¼Œè®©ç‚®ç®¡è¿ç»­è½¬åŠ¨ï¼Œè€Œééšæœºä¹±è½¬
        if ai_action == 0:
            game_action = random.choice(self.MOVE_ACTION_LIST)
        else:
            game_action = ACTION_GUN_LEFT if random.random() < 0.6 else ACTION_GUN_RIGHT
        return game_action, ai_action, probs[0, ai_action].item()

    def get_best_action(self, state):
        """æµ‹è¯•ç”¨ï¼šç„å‡†åŠ¨ä½œ7:3æ–¹å‘åå¥½ï¼Œç‚®ç®¡è¿ç»­è½¬ä¸æŠ–åŠ¨"""
        s = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            probs = self(s)
        ai_action = torch.argmax(probs, dim=1).item()
        if ai_action == 0:
            return random.choice(self.MOVE_ACTION_LIST)
        else:
            return ACTION_GUN_LEFT if random.random() < 0.7 else ACTION_GUN_RIGHT

class PPO_Critic(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),  # æ›¿æ¢LeakyReLU
            nn.Linear(64, 32),
            nn.ReLU(),  # æ›¿æ¢LeakyReLU
            nn.Linear(32, 1)
        ).to(DEVICE)
        # æµ…å±‚ç½‘ç»œåˆå§‹åŒ–ä¼˜åŒ–ï¼šåç½®ç½®0
        for m in self.net.modules():
            if isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0.0)

    def forward(self, x):
        return self.net(x.to(DEVICE))

# =========================================================
# GAN åˆ¤åˆ«å™¨ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šLeakyReLUâ†’ReLUï¼‰
# =========================================================
class GAILGAN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.discriminator = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),  # æ›¿æ¢LeakyReLU
            nn.Linear(128, 1)
        ).to(DEVICE)
        self.optimizer = optim.Adam(self.discriminator.parameters(), lr=GAN_LR)
        # åˆå§‹åŒ–ä¼˜åŒ–ï¼šåç½®ç½®0
        for m in self.discriminator.modules():
            if isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0.0)
    
    def forward(self, x):
        return self.discriminator(x.to(DEVICE))
    
    def train_step(self, agent_batch, expert_batch):
        expert_logits = self(expert_batch)
        agent_logits = self(agent_batch)
        loss_expert = F.binary_cross_entropy_with_logits(expert_logits, torch.ones_like(expert_logits).to(DEVICE))
        loss_agent = F.binary_cross_entropy_with_logits(agent_logits, torch.zeros_like(agent_logits).to(DEVICE))
        loss = 0.5 * (loss_expert + loss_agent)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        with torch.no_grad():
            gan_reward = -torch.log(torch.sigmoid(agent_logits) + 1e-8).mean().item()
        return loss.item(), gan_reward

# =========================================================
# PPO-GAN æ™ºèƒ½ä½“
# =========================================================
class PPO_GAN_Simple:
    def __init__(self):
        self.actor = PPO_Actor(STATE_DIM, ACTION_DIM)
        self.critic = PPO_Critic(STATE_DIM)
        self.ppo_opt = optim.Adam(list(self.actor.parameters()) + list(self.critic.parameters()), lr=PPO_LR)
        self.ppo_memory = PPOMemory(MEMORY_CAPACITY)
        self.demo_memory = DemoMemory(DEMO_MEMORY_CAPACITY, DEMO_VEC_DIM)
        self.gan = GAILGAN(DEMO_VEC_DIM)
        self.reset_combat_state()

    def reset_combat_state(self):
        self.no_kill_step = 0
        self.last_action = -1
        self.last_player_pos = (0, 0)

    def one_hot(self, actions):
        oh = torch.zeros(len(actions), ACTION_DIM).to(DEVICE)
        oh[range(len(actions)), actions] = 1.0
        return oh
    
    def compute_gae(self, r, d, v, nv):
        gae = 0
        adv = torch.zeros_like(r).to(DEVICE)
        for i in reversed(range(len(r))):
            delta = r[i] + GAMMA * nv[i] * (1 - d[i]) - v[i]
            gae = delta + GAMMA * LAMBDA * (1 - d[i]) * gae
            adv[i] = gae
        return adv.clamp(-1.0, 1.0)

    def train_ppo(self):
        """PPOè®­ç»ƒï¼šç´¢å¼•å·²åŒ¹é…ï¼ˆaæ˜¯0/1ï¼‰ï¼Œæ— è¶Šç•Œ"""
        batch = self.ppo_memory.sample(BATCH_SIZE)
        if batch is None:
            return 0.0
        s, a, r, ns, d, old_p = batch
        with torch.no_grad():
            v = self.critic(s)
            nv = self.critic(ns)
        adv = self.compute_gae(r, d, v, nv)
        target_v = adv + v
        probs = self.actor(s)
        new_p = probs.gather(1, a)
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1).mean()
        ratio = torch.exp(torch.log(new_p + 1e-8) - torch.log(old_p + 1e-8))
        surr1 = ratio * adv
        surr2 = torch.clamp(ratio, 1 - EPS_CLIP, 1 + EPS_CLIP) * adv
        actor_loss = -torch.min(surr1, surr2).mean()
        critic_loss = F.mse_loss(self.critic(s), target_v)
        total_loss = actor_loss + 0.5 * critic_loss - ENT_COEF * entropy
        self.ppo_opt.zero_grad()
        total_loss.backward()
        self.ppo_opt.step()
        return total_loss.item()

    def save(self, ep):
        save_path = f"./tank_ai_models_simple/ppo_gan_simple_ep{ep}.pth"
        torch.save({"actor": self.actor.state_dict(), "critic": self.critic.state_dict()}, save_path)
        print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜ï¼š{save_path}")

    def load(self, model_path):
        checkpoint = torch.load(model_path, map_location=DEVICE)
        self.actor.load_state_dict(checkpoint["actor"])
        self.critic.load_state_dict(checkpoint["critic"])
        print(f"âœ… åŠ è½½æ¨¡å‹ï¼š{model_path}")

# =========================================================
# å·¥å…·å‡½æ•°ï¼ˆæ–°å¢å‘æ•Œç§»åŠ¨åˆ¤å®šï¼‰
# =========================================================
def get_nearest_enemy(game):
    enemies_alive = [e for e in game.enemies if e.alive]
    if not enemies_alive:
        return None
    distances = [math.hypot(e.x - game.player.x, e.y - game.player.y) for e in enemies_alive]
    return enemies_alive[np.argmin(distances)]

def raycast_obstacle(game, start_pos, end_pos):
    sx, sy = start_pos
    ex, ey = end_pos
    dx = ex - sx
    dy = ey - sy
    dist = math.hypot(dx, dy)
    if dist == 0:
        return True
    step_x = (dx / dist) * RAYCAST_STEP
    step_y = (dy / dist) * RAYCAST_STEP
    current_x, current_y = sx, sy
    for _ in range(int(dist // RAYCAST_STEP) + 1):
        current_x += step_x
        current_y += step_y
        for wall in game.walls:
            wall_rect = pygame.Rect(wall[0], wall[1], WALL_SIZE, WALL_SIZE)
            if wall_rect.collidepoint(current_x, current_y):
                return True
    return False

def is_in_direct_shot_position(game):
    enemy = get_nearest_enemy(game)
    if not enemy or not game.player.alive:
        return False
    return not raycast_obstacle(game, (game.player.x, game.player.y), (enemy.x, enemy.y))

def is_dodging_bullet(game):
    if not hasattr(game, "bullets") or len(game.bullets) == 0:
        return True
    player_x, player_y = game.player.x, game.player.y
    for bullet in game.bullets:
        if not bullet.is_player_bullet and math.hypot(bullet.x - player_x, bullet.y - player_y) <= BULLET_DODGE_DIST:
            return False
    return True

def is_enemy_exposed(game):
    enemy = get_nearest_enemy(game)
    if not enemy:
        return False
    return math.hypot(enemy.x - game.player.x, enemy.y - game.player.y) < CLOSE_DIST_THRESH

def calculate_aim_error(game):
    enemy = get_nearest_enemy(game)
    if not enemy:
        return 1.0
    dx = enemy.x - game.player.x
    dy = enemy.y - game.player.y
    target_angle = math.atan2(-dy, dx) % (2 * math.pi)
    current_angle = game.player.aim_angle % (2 * math.pi)
    angle_error = abs(current_angle - target_angle)
    angle_error = min(angle_error, 2 * math.pi - angle_error)
    return angle_error / math.pi  # å½’ä¸€åŒ–åˆ°0~1

# æ ¸å¿ƒæ–°å¢ï¼šåˆ¤å®šæ˜¯å¦å‘æ•Œäººç§»åŠ¨
def is_toward_enemy(game, last_player_pos):
    enemy = get_nearest_enemy(game)
    if not enemy or not game.player.alive:
        return False
    last_x, last_y = last_player_pos
    dist_last = math.hypot(enemy.x - last_x, enemy.y - last_y)
    dist_current = math.hypot(enemy.x - game.player.x, enemy.y - game.player.y)
    return (dist_last - dist_current) > 1e-3  # è·ç¦»å‡å°‘=å‘æ•Œç§»åŠ¨

# =========================================================
# å¥–åŠ±å‡½æ•°ï¼ˆå®Œå…¨é‡å†™ï¼šé›¶é—¨æ§›æ¢¯åº¦ç„å‡†+å‘æ•Œç§»åŠ¨å¼•å¯¼ï¼‰
# =========================================================
def get_env_reward(game, action, agent):
    final_reward = 0.0
    enemy = get_nearest_enemy(game)
    enemy_visible = enemy is not None and game.player.alive
    current_player_pos = (game.player.x, game.player.y)

    # 1. ç„å‡†åŠ¨ä½œå¥–åŠ±ï¼ˆæ ¸å¿ƒï¼šé›¶é—¨æ§›+æ¢¯åº¦åŒ–ï¼Œåªè¦æœ‰æ•Œäººå°±ç»™å¥–åŠ±ï¼‰
    if action in [ACTION_GUN_LEFT, ACTION_GUN_RIGHT] and enemy_visible:
        aim_error = calculate_aim_error(game)
        # é›¶é—¨æ§›æ¢¯åº¦å¥–åŠ±ï¼šè¯¯å·®è¶Šå°ï¼Œå¥–åŠ±è¶Šé«˜
        final_reward += REWARD_AIM_GRADIENT * (1 - aim_error)
        # è¾ƒå¥½ç„å‡†å¥–åŠ±ï¼ˆè¯¯å·®<0.3ï¼Œæ˜“è§¦å‘ï¼‰
        if aim_error < AIM_GOOD_THRESH:
            final_reward += REWARD_AIM_GOOD
        # å®Œç¾ç„å‡†å¥–åŠ±ï¼ˆè¯¯å·®<0.2ï¼Œå åŠ é«˜é¢å¥–åŠ±ï¼‰
        if aim_error < AIM_PERFECT_THRESH:
            final_reward += REWARD_AIM_PERFECT

    # 2. ç§»åŠ¨åŠ¨ä½œå¥–åŠ±ï¼ˆå¼•å¯¼æœ‰æ„è¯†è·‘ä½ï¼šä¼˜å…ˆå‘æ•Œç§»åŠ¨ï¼‰
    if action in [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT] and enemy_visible:
        # æ ¸å¿ƒï¼šå‘æ•Œäººç§»åŠ¨å°±ç»™å¥–åŠ±ï¼Œå¼•å¯¼AIä¸»åŠ¨é è¿‘
        if is_toward_enemy(game, agent.last_player_pos):
            final_reward += REWARD_TOWARD_ENEMY
        # ç›´å°„ä½å¥–åŠ±ï¼ˆæ¬¡è¦ï¼‰
        if is_in_direct_shot_position(game):
            final_reward += REWARD_DIRECT_SHOT_POS
        # èº²å­å¼¹å¥–åŠ±ï¼ˆæœ€æ¬¡è¦ï¼‰
        if is_dodging_bullet(game):
            final_reward += REWARD_DODGE_BULLET

    # 3. æƒ©ç½šé¡¹ï¼ˆå¤§å¹…å¼±åŒ–ï¼Œä¸å‹åˆ¶æ¢ç´¢ï¼‰
    if enemy_visible:
        agent.no_kill_step += 1
        if agent.no_kill_step >= NO_KILL_STEP_THRESH:
            final_reward -= PUNISH_NO_KILL
            agent.no_kill_step = NO_KILL_STEP_THRESH - 10
    # é‡å¤åŠ¨ä½œæƒ©ç½šï¼ˆå…è®¸è¿ç»­ç„å‡†/å‘æ•Œç§»åŠ¨ï¼‰
    if agent.last_action == action and agent.last_action != -1:
        final_reward -= PUNISH_IDLE
    # è¢«å‡»ä¸­æƒ©ç½šï¼ˆé™ä½ï¼Œä¸æ€•å¤±è¯¯ï¼‰
    if hasattr(game.player, 'been_hit') and game.player.been_hit:
        final_reward -= PUNISH_BEEN_HIT
        game.player.been_hit = False

    # 4. è‡ªåŠ¨å¼€ç«ç»‘å®šç„å‡†ç²¾åº¦ï¼šåªæœ‰è¾ƒå¥½ç„å‡†æ‰å¼€ç«ï¼ˆé¿å…æ— è„‘å°„å‡»ï¼‰
    if enemy_visible:
        aim_error = calculate_aim_error(game)
        game.player.auto_shoot = aim_error < AIM_GOOD_THRESH
    else:
        game.player.auto_shoot = False

    # æ›´æ–°ä»£ç†çŠ¶æ€
    agent.last_action = action
    agent.last_player_pos = current_player_pos
    return np.clip(final_reward, -10, 20)

# =========================================================
# ç”Ÿæˆä¸“å®¶ç»éªŒ
# =========================================================
def generate_demo_data(demo_memory, demo_num=300):  # å‡å°‘é‡‡é›†æ•°é‡ï¼Œå¿«é€Ÿå®Œæˆ
    print("\nğŸ® ç”Ÿæˆä¸“å®¶ç»éªŒ | æŒ‰é”®ï¼šWASD=ç§»åŠ¨ | â†â†’=ç„å‡† | ESCé€€å‡º")
    print(f"ğŸ¯ ç›®æ ‡é‡‡é›†ï¼š{demo_num}æ¡æœ‰æ•ˆç»éªŒï¼ˆé‡ç‚¹å¤šè½¬ç‚®ç®¡ç„å‡†ï¼ï¼‰")
    game = TankGame(render=True)
    state = game.reset()
    clock = pygame.time.Clock()
    running = True

    while running and len(demo_memory) < demo_num:
        clock.tick(60)
        action = 0
        for event in pygame.event.get():
            if event.type == pygame.QUIT or (event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE):
                running = False
                break
        if not running:
            break
        # æ‰‹åŠ¨æ“ä½œæ˜ å°„AIåŠ¨ä½œç±»å‹
        keys = pygame.key.get_pressed()
        ai_action_type = 0
        if keys[pygame.K_w] or keys[pygame.K_s] or keys[pygame.K_a] or keys[pygame.K_d]:
            ai_action_type = 0
            action = ACTION_UP if keys[pygame.K_w] else ACTION_DOWN if keys[pygame.K_s] else ACTION_LEFT if keys[pygame.K_a] else ACTION_RIGHT
        elif keys[pygame.K_LEFT] or keys[pygame.K_RIGHT]:
            ai_action_type = 1
            action = ACTION_GUN_LEFT if keys[pygame.K_LEFT] else ACTION_GUN_RIGHT
        # é‡‡é›†ç»éªŒï¼ˆæ”¾å®½æ¡ä»¶ï¼Œåªè¦å­˜æ´»å°±é‡‡é›†ï¼Œé‡ç‚¹æ˜¯ç„å‡†åŠ¨ä½œï¼‰
        if game.player.alive:
            state_np = np.asarray(state, dtype=np.float32).flatten()
            action_one_hot = np.eye(ACTION_DIM, dtype=np.float32)[ai_action_type]
            demo_vec = np.concatenate([state_np, action_one_hot])
            demo_memory.add(demo_vec)
            if len(demo_memory) % 50 == 0:
                print(f"ğŸ“ˆ å·²é‡‡é›†ï¼š{len(demo_memory)}/{demo_num} æ¡")
        # æ‰§è¡ŒåŠ¨ä½œ
        game.do_action(action)
        game.player.auto_shoot = True
        game.step()
        state = game.get_state()
        if game.game_over:
            state = game.reset()
            print(f"ğŸ”„ æ¸¸æˆé‡ç½® | ç»§ç»­é‡‡é›†ç»éªŒ...")
    # ä¿å­˜ç»éªŒ
    demo_memory.save_to_npy("./tank_demo_data_simple/demo_memory_simple.npy")
    pygame.quit()
    print(f"\nâœ… ä¸“å®¶ç»éªŒç”Ÿæˆå®Œæˆï¼å®é™…é‡‡é›†ï¼š{len(demo_memory)}æ¡")

# =========================================================
# è®­ç»ƒå…¥å£
# =========================================================
def train_ai(load_model_path=None):
    pygame.init()
    game = TankGame(render=RENDER_TRAIN)
    agent = PPO_GAN_Simple()

    # åŠ è½½/é‡‡é›†ä¸“å®¶ç»éªŒ
    demo_path = "./tank_demo_data_simple/demo_memory_simple.npy"
    if os.path.exists(demo_path):
        agent.demo_memory.load_from_npy(demo_path)
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°ä¸“å®¶ç»éªŒï¼Œå¼€å§‹æ‰‹åŠ¨é‡‡é›†...")
        generate_demo_data(agent.demo_memory, 300)  # åªé‡‡é›†300æ¡ï¼Œå¿«é€Ÿå®Œæˆ
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if load_model_path and os.path.exists(load_model_path):
        agent.load(load_model_path)
    # ç»´åº¦æ ¡éªŒ
    assert len(game.get_state()) == STATE_DIM, f"çŠ¶æ€ç»´åº¦ä¸åŒ¹é…ï¼æ¸¸æˆ{len(game.get_state())} vs é…ç½®{STATE_DIM}"
    print(f"\nğŸš€ æ­£å¼å¼€å§‹è®­ç»ƒ | æ€»è½®æ•°ï¼š{TRAIN_EPISODES} | GANå¥–åŠ±æƒé‡ï¼š{GAN_REWARD_WEIGHT}")
    print(f"ğŸ“Œ æ ¸å¿ƒå¥–åŠ±ï¼šé›¶é—¨æ§›ç„å‡†+{REWARD_AIM_GRADIENT} | å‘æ•Œç§»åŠ¨+{REWARD_TOWARD_ENEMY}")
    pbar = tqdm(range(1, TRAIN_EPISODES + 1), desc="è®­ç»ƒè¿›åº¦")

    for ep in pbar:
        state = game.reset()
        agent.reset_combat_state()
        total_reward = 0.0
        step = 0
        ppo_loss_sum = 0.0
        ppo_train_count = 0
        gan_reward_sum = 0.0

        while step < MAX_STEP and not game.game_over:
            step += 1
            # è·å–åŠ¨ä½œï¼ˆè®­ç»ƒæœŸå¸¦ç„å‡†æ–¹å‘åå¥½ï¼‰
            game_action, ai_action, action_prob = agent.actor.get_action(state)
            game.do_action(game_action)
            _, done = game.step()
            # è®¡ç®—æ ¸å¿ƒç¯å¢ƒå¥–åŠ±ï¼ˆé›¶é—¨æ§›ç„å‡†+å‘æ•Œå¼•å¯¼ï¼‰
            base_reward = get_env_reward(game, game_action, agent)
            next_state = game.get_state()

            # GANå¥–åŠ±èåˆï¼ˆå°æƒé‡ï¼Œè¾…åŠ©è®­ç»ƒï¼‰
            gan_reward = 0.0
            if step % GAN_UPDATE_INTERVAL == 0 and len(agent.ppo_memory) >= BATCH_SIZE:
                batch = agent.ppo_memory.sample(BATCH_SIZE)
                if batch is not None:
                    s_ppo, a_ppo, _, _, _, _ = batch
                    ai_action_onehot = agent.one_hot(a_ppo.squeeze(1))
                    agent_batch = torch.cat([s_ppo, ai_action_onehot], dim=-1)
                    expert_batch = agent.demo_memory.sample(BATCH_SIZE)
                    gan_loss, gan_reward = agent.gan.train_step(agent_batch, expert_batch)
                    gan_reward_sum += gan_reward
            # æ€»å¥–åŠ± = ç¯å¢ƒå¥–åŠ± + å°æƒé‡GANå¥–åŠ±
            total_reward_step = base_reward + GAN_REWARD_WEIGHT * gan_reward

            # å­˜å‚¨ç»éªŒ
            agent.ppo_memory.add(state, ai_action, total_reward_step, next_state, done, action_prob)
            total_reward += total_reward_step

            # è®­ç»ƒPPO
            if step % 3 == 0:
                ppo_loss = agent.train_ppo()
                ppo_loss_sum += ppo_loss
                ppo_train_count += 1
            
            # æ›´æ–°çŠ¶æ€
            state = next_state

        # è¿›åº¦æ¡å±•ç¤º
        avg_ppo_loss = ppo_loss_sum / max(ppo_train_count, 1)
        avg_gan_reward = gan_reward_sum / max(step // GAN_UPDATE_INTERVAL, 1)
        pbar.set_postfix({
            "æ€»å¥–åŠ±": f"{total_reward:.2f}",
            "PPOæŸå¤±": f"{avg_ppo_loss:.4f}",
            "å¹³å‡GANå¥–åŠ±": f"{avg_gan_reward:.3f}",
            "ç»éªŒæ± ": f"{len(agent.ppo_memory)}/{MEMORY_CAPACITY}"
        })
        # ä¿å­˜æ¨¡å‹
        if ep % SAVE_INTERVAL == 0:
            agent.save(ep)

    # è®­ç»ƒå®Œæˆ
    agent.save(TRAIN_EPISODES)
    pygame.quit()
    print(f"\nğŸ‰ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³ï¼š./tank_ai_models_simple/ppo_gan_simple_ep{TRAIN_EPISODES}.pth")

# =========================================================
# æµ‹è¯•å…¥å£
# =========================================================
def test_ai(model_path):
    pygame.init()
    game = TankGame(render=RENDER_TEST)
    agent = PPO_GAN_Simple()
    agent.load(model_path)

    state = game.reset()
    agent.reset_combat_state()
    clock = pygame.time.Clock()
    total_reward = 0.0
    step = 0
    kill_num = 0
    print(f"\nğŸ® AIæµ‹è¯•å¯åŠ¨ | æ¨¡å‹ï¼š{model_path} | æœ€å¤§æ­¥æ•°ï¼š{MAX_STEP}")
    print(f"ğŸ’¡ æŒ‰ Q æˆ– å…³é—­çª—å£ é€€å‡ºæµ‹è¯• | ç„å‡†åå¥½ï¼š70%å·¦ | 30%å³")

    while step < MAX_STEP and not game.game_over:
        step += 1
        clock.tick(60)
        # è°ƒç”¨å¸¦æ–¹å‘åå¥½çš„æœ€ä¼˜åŠ¨ä½œ
        action = agent.actor.get_best_action(state)
        game.do_action(action)
        game.step()
        reward = get_env_reward(game, action, agent)
        total_reward += reward
        state = game.get_state()
        kill_num = game.score // 70 if game.score > 0 else 0

        # é€€å‡ºç›‘å¬
        for event in pygame.event.get():
            if event.type == pygame.QUIT or (event.type == pygame.KEYDOWN and event.key == pygame.K_q):
                pygame.quit()
                print(f"\nğŸ›‘ æµ‹è¯•æ‰‹åŠ¨é€€å‡º")
                return

    # æµ‹è¯•ç»“æœ
    pygame.quit()
    print(f"\nâœ… æµ‹è¯•ç»“æŸ | æœ€ç»ˆç»Ÿè®¡ï¼š")
    print(f"ğŸ“Š æ­¥æ•°ï¼š{step} | æ€»å¥–åŠ±ï¼š{total_reward:.2f} | æ€»å¾—åˆ†ï¼š{game.score}")
    print(f"ğŸ† å‡»æ€æ•°ï¼š{kill_num} | å‰©ä½™ç”Ÿå‘½ï¼š{game.player.lives}")
    print(f"ğŸ“ˆ æµ‹è¯•ç»“æœï¼š{'èƒœåˆ©' if kill_num >=8 else 'å¤±è´¥'}ï¼ˆèƒœåˆ©æ¡ä»¶ï¼šå‡»æ€â‰¥8ï¼‰")

# =========================================================
# ä¸»å‡½æ•°ï¼ˆè®­ç»ƒ/æµ‹è¯•ä¸€é”®åˆ‡æ¢ï¼‰
# =========================================================
if __name__ == "__main__":
    TRAIN_MODE = True  # True=è®­ç»ƒï¼ŒFalse=æµ‹è¯•
    PRETRAIN_MODEL = None  # ç»§ç»­è®­ç»ƒçš„æ¨¡å‹è·¯å¾„ï¼ŒNoneåˆ™ä»å¤´è®­ç»ƒ
    TEST_MODEL = "./tank_ai_models_simple/ppo_gan_simple_ep1200.pth"  # æµ‹è¯•æ¨¡å‹è·¯å¾„

    if TRAIN_MODE:
        train_ai(load_model_path=PRETRAIN_MODEL)
    else:
        if not os.path.exists(TEST_MODEL):
            print(f"âŒ æµ‹è¯•æ¨¡å‹ä¸å­˜åœ¨ï¼š{TEST_MODEL}")
            print(f"ğŸ’¡ è¯·å…ˆæ‰§è¡Œè®­ç»ƒæ¨¡å¼ç”Ÿæˆæ¨¡å‹")
        else:
            test_ai(model_path=TEST_MODEL)