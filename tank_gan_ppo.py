# tankgame_working_train_fixed.py
"""
ğŸš€ å¦å…‹æ¸¸æˆAI - çœŸæ­£æœ‰æ•ˆçš„å·¥ä½œè®­ç»ƒè„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰
åŸºäºè¯Šæ–­ç»“æœï¼šè§„åˆ™AIèƒ½å·¥ä½œï¼Œæ‰€ä»¥ç¥ç»ç½‘ç»œåº”è¯¥ä¹Ÿèƒ½å­¦ä¹ 
ä¿®å¤äº†Pygameå­—ä½“åˆå§‹åŒ–é—®é¢˜
"""

import pygame
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import math
import random
from collections import deque
import time
import os

# ============ åˆå§‹åŒ– ============
pygame.init()
pygame.font.init()

# ============ å¯¼å…¥æ¸¸æˆ ============
from tankgame import TankGame, ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, ACTION_GUN_LEFT, ACTION_GUN_RIGHT

# ============ ä¿®å¤ï¼šæ­£ç¡®çš„åŠ¨ä½œæ˜ å°„ ============
# æ ¹æ®tankgame.pyä¸­çš„å®é™…å¸¸é‡
ACTION_MAP = {
    0: None,              # æ— åŠ¨ä½œ
    1: ACTION_UP,         # ä¸Š
    2: ACTION_DOWN,       # ä¸‹
    3: ACTION_LEFT,       # å·¦
    4: ACTION_RIGHT,      # å³
    5: ACTION_GUN_LEFT,   # ç‚®ç®¡å·¦è½¬
    6: ACTION_GUN_RIGHT   # ç‚®ç®¡å³è½¬
}

# åå‘æ˜ å°„ï¼šæ¸¸æˆåŠ¨ä½œ -> ç½‘ç»œåŠ¨ä½œç´¢å¼•
GAME_ACTION_TO_IDX = {v: k for k, v in ACTION_MAP.items() if v is not None}

# ============ è¶…å‚æ•° ============
STATE_DIM = 14
ACTION_DIM = 7  # 0-6ï¼Œä½†0æ˜¯æ— åŠ¨ä½œ

# è®­ç»ƒå‚æ•°
LEARNING_RATE = 0.001
GAMMA = 0.99
BATCH_SIZE = 64
MEMORY_SIZE = 10000
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995

# ============ ç»éªŒå›æ”¾ç¼“å†²åŒº ============
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, min(len(self.buffer), batch_size))
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states), np.array(actions), np.array(rewards), 
                np.array(next_states), np.array(dones))
    
    def __len__(self):
        return len(self.buffer)

# ============ èªæ˜çš„ç¥ç»ç½‘ç»œ ============
class SmartAIModel(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        
        # ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, action_dim)
        )
        
        # åˆå§‹åŒ–åå‘ç„å‡†åŠ¨ä½œï¼ˆ5,6ï¼‰
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡ï¼Œåå‘ç„å‡†åŠ¨ä½œ"""
        for layer in self.net:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
        
        # æœ€åå±‚åå‘ç‚®ç®¡è½¬åŠ¨
        with torch.no_grad():
            last_layer = self.net[-1]
            last_layer.weight[5] += 0.5  # ç‚®å·¦è½¬
            last_layer.weight[6] += 0.5  # ç‚®å³è½¬
    
    def forward(self, x):
        return self.net(x)

# ============ DQN Agent ============
class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ç½‘ç»œ
        self.policy_net = SmartAIModel(state_dim, action_dim).to(self.device)
        self.target_net = SmartAIModel(state_dim, action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)
        
        # ç»éªŒå›æ”¾
        self.memory = ReplayBuffer(MEMORY_SIZE)
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = EPSILON_START
        self.steps_done = 0
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_kills = []
    
    def select_action(self, state, game=None):
        """é€‰æ‹©åŠ¨ä½œï¼Œå¸¦æ¢ç´¢"""
        self.steps_done += 1
        
        # è¡°å‡epsilon
        self.epsilon = max(EPSILON_END, 
                          EPSILON_START * (EPSILON_DECAY ** self.steps_done))
        
        # epsilon-greedyç­–ç•¥
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œï¼Œä½†åå‘æœ‰ç”¨çš„åŠ¨ä½œ
            if random.random() < 0.6:  # 60%é€‰æ‹©ç„å‡†åŠ¨ä½œ
                return random.choice([5, 6])  # ç‚®ç®¡è½¬åŠ¨
            else:
                return random.choice([1, 2, 3, 4])  # ç§»åŠ¨
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.policy_net(state_tensor)
                
                # å¢å¼ºç„å‡†åŠ¨ä½œçš„Qå€¼ï¼ˆå¦‚æœæ¥è¿‘æ•Œäººï¼‰
                if game and game.enemies:
                    enemy = game.enemies[0]
                    dx = enemy.x - game.player.x
                    dy = enemy.y - game.player.y
                    target_angle = math.atan2(-dy, dx)
                    current_angle = game.player.aim_angle
                    
                    angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
                    angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
                    
                    # å¦‚æœæ²¡ç„å‡†å¥½ï¼Œå¢å¼ºç„å‡†åŠ¨ä½œ
                    if angle_diff > 0.3:  # å¤§äº17åº¦
                        q_values[0, 5] += 2.0  # ç‚®å·¦è½¬
                        q_values[0, 6] += 2.0  # ç‚®å³è½¬
                
                return q_values.argmax(dim=1).item()
    
    def optimize_model(self):
        """ä¼˜åŒ–æ¨¡å‹"""
        if len(self.memory) < BATCH_SIZE:
            return
        
        # é‡‡æ ·
        states, actions, rewards, next_states, dones = self.memory.sample(BATCH_SIZE)
        
        # è½¬ä¸ºtensor
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q = self.policy_net(states).gather(1, actions)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1, keepdim=True)[0]
            target_q = rewards + (1 - dones) * GAMMA * next_q
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_q, target_q)
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_net(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_net.load_state_dict(self.policy_net.state_dict())
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'target_net_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'steps_done': self.steps_done,
            'episode_rewards': self.episode_rewards,
            'episode_kills': self.episode_kills
        }, path)
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.steps_done = checkpoint['steps_done']
        self.episode_rewards = checkpoint['episode_rewards']
        self.episode_kills = checkpoint['episode_kills']

# ============ å¢å¼ºçš„å¥–åŠ±å‡½æ•° ============
def calculate_reward(game, prev_score, prev_enemies_count):
    """è®¡ç®—å¢å¼ºçš„å¥–åŠ±"""
    reward = 0.0
    
    # 1. å‡»æ€å¥–åŠ±ï¼ˆæœ€é‡è¦ï¼‰
    current_score = game.score
    if current_score > prev_score:
        kill_reward = 100.0  # å¤§å¹…å¥–åŠ±å‡»æ€
        reward += kill_reward
    
    # 2. å‡»ä¸­æ•Œäººå¥–åŠ±
    # è¿™é‡Œéœ€è¦æ ¹æ®æ¸¸æˆå®é™…æƒ…å†µè°ƒæ•´
    
    # 3. ç”Ÿå­˜å¥–åŠ±
    reward += 0.1  # æ¯æ­¥ç”Ÿå­˜å¥–åŠ±
    
    # 4. ç„å‡†è´¨é‡å¥–åŠ±
    if game.enemies:
        enemy = game.enemies[0]
        dx = enemy.x - game.player.x
        dy = enemy.y - game.player.y
        target_angle = math.atan2(-dy, dx)
        current_angle = game.player.aim_angle
        
        angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
        angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
        
        # ç„å‡†è¶Šå¥½å¥–åŠ±è¶Šé«˜
        aim_reward = 0.5 * (1.0 - angle_diff / math.pi)
        reward += aim_reward
    
    # 5. æƒ©ç½šè¢«å‡»ä¸­
    # è¿™é‡Œéœ€è¦æ ¹æ®æ¸¸æˆå®é™…æƒ…å†µè°ƒæ•´
    
    # 6. æƒ©ç½šæ— æ•ˆå¼€ç«
    if game.player.auto_shoot:
        reward -= 0.01  # è½»å¾®æƒ©ç½šå¼€ç«æ¶ˆè€—
    
    return reward

# ============ è®­ç»ƒå¾ªç¯ ============
def train_dqn():
    print("ğŸš€ å¼€å§‹DQNè®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºæ¸¸æˆå’Œæ™ºèƒ½ä½“
    game = TankGame(render=False)
    agent = DQNAgent(STATE_DIM, ACTION_DIM)
    
    # è®­ç»ƒå‚æ•°
    num_episodes = 2000
    target_update = 10  # æ¯10è½®æ›´æ–°ç›®æ ‡ç½‘ç»œ
    save_interval = 100  # æ¯100è½®ä¿å­˜æ¨¡å‹
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs("./checkpoints", exist_ok=True)
    
    # é¢„å¡«å……ç»éªŒæ± 
    print("é¢„å¡«å……ç»éªŒæ± ...")
    while len(agent.memory) < BATCH_SIZE * 2:
        state = game.reset()
        prev_score = game.score
        
        for step in range(50):
            # éšæœºåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
            action = random.choice([1, 2, 3, 4, 5, 6])
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if action in ACTION_MAP and ACTION_MAP[action]:
                game.do_action(ACTION_MAP[action])
            
            # è‡ªåŠ¨å¼€ç«é€»è¾‘
            if action in [5, 6] and game.enemies:
                enemy = game.enemies[0]
                dx = enemy.x - game.player.x
                dy = enemy.y - game.player.y
                target_angle = math.atan2(-dy, dx)
                current_angle = game.player.aim_angle
                
                angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
                angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
                
                if angle_diff / math.pi < 0.3:
                    game.player.auto_shoot = True
            
            # æ¸¸æˆæ­¥è¿›
            game.step()
            next_state = game.get_state()
            
            # è®¡ç®—å¥–åŠ±
            reward = calculate_reward(game, prev_score, 0)
            prev_score = game.score
            
            # å­˜å‚¨ç»éªŒ
            done = game.game_over
            agent.memory.push(state, action, reward, next_state, done)
            
            state = next_state
            
            if done:
                break
    
    print(f"ç»éªŒæ± é¢„å¡«å……å®Œæˆ: {len(agent.memory)} æ¡ç»éªŒ")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    print("\nå¼€å§‹ä¸»è®­ç»ƒå¾ªç¯...")
    
    for episode in range(num_episodes):
        # é‡ç½®ç¯å¢ƒ
        state = game.reset()
        prev_score = game.score
        episode_reward = 0
        episode_kills = 0
        
        # è·å–åˆå§‹å‡»æ€æ•°
        initial_kills = game.score // 70 if hasattr(game, 'score') else 0
        
        for step in range(300):  # æ¯å›åˆæœ€å¤š300æ­¥
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, game)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if action in ACTION_MAP and ACTION_MAP[action]:
                game.do_action(ACTION_MAP[action])
            
            # æ™ºèƒ½è‡ªåŠ¨å¼€ç«
            if game.enemies and action in [5, 6]:
                enemy = game.enemies[0]
                dx = enemy.x - game.player.x
                dy = enemy.y - game.player.y
                target_angle = math.atan2(-dy, dx)
                current_angle = game.player.aim_angle
                
                angle_diff = abs((target_angle - current_angle) % (2 * math.pi))
                angle_diff = min(angle_diff, 2 * math.pi - angle_diff)
                
                # åªæœ‰ç„å‡†å¾—æ¯”è¾ƒå¥½æ—¶æ‰å¼€ç«
                if angle_diff / math.pi < 0.2:  # 20%è¯¯å·®å†…
                    game.player.auto_shoot = True
                else:
                    game.player.auto_shoot = False
            else:
                game.player.auto_shoot = False
            
            # æ¸¸æˆæ­¥è¿›
            game.step()
            next_state = game.get_state()
            
            # è®¡ç®—å¥–åŠ±
            reward = calculate_reward(game, prev_score, 0)
            prev_score = game.score
            
            # ç»Ÿè®¡å‡»æ€
            current_kills = game.score // 70 if hasattr(game, 'score') else 0
            if current_kills > episode_kills:
                episode_kills = current_kills
            
            # å­˜å‚¨ç»éªŒ
            done = game.game_over or step == 299
            agent.memory.push(state, action, reward, next_state, done)
            
            # ä¼˜åŒ–æ¨¡å‹
            loss = agent.optimize_model()
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward
            
            if done:
                break
        
        # è®°å½•ç»Ÿè®¡
        agent.episode_rewards.append(episode_reward)
        agent.episode_kills.append(episode_kills)
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % target_update == 0:
            agent.update_target_net()
        
        # æ‰“å°è¿›åº¦
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(agent.episode_rewards[-50:])
            avg_kills = np.mean(agent.episode_kills[-50:])
            kill_rate = sum(agent.episode_kills[-50:]) / 50 * 100
            
            print(f"å›åˆ {episode+1:4d} | "
                  f"å¹³å‡å¥–åŠ±: {avg_reward:6.1f} | "
                  f"å¹³å‡å‡»æ€: {avg_kills:4.1f} | "
                  f"å‡»æ€ç‡: {kill_rate:5.1f}% | "
                  f"Epsilon: {agent.epsilon:.3f}")
        
        # ä¿å­˜æ¨¡å‹
        if (episode + 1) % save_interval == 0:
            agent.save_model(f"./checkpoints/model_episode_{episode+1}.pth")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save_model("./tank_ai_final.pth")
    print("\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸º tank_ai_final.pth")
    
    pygame.quit()
    return agent

# ============ æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ ============
def test_trained_model(model_path=None):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆä¿®å¤ç‰ˆï¼šç¡®ä¿Pygameå­—ä½“åˆå§‹åŒ–ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•AIæ€§èƒ½")
    print("=" * 60)
    
    # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿Pygameå’Œå­—ä½“æ¨¡å—å·²åˆå§‹åŒ–
    try:
        pygame.init()
        pygame.font.init()
    except:
        pass
    
    game = TankGame(render=True)
    
    if model_path and os.path.exists(model_path):
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        agent = DQNAgent(STATE_DIM, ACTION_DIM)
        agent.load_model(model_path)
        agent.epsilon = 0.01  # æµ‹è¯•æ—¶ç”¨å¾ˆå°çš„æ¢ç´¢ç‡
    else:
        print("ä½¿ç”¨æ–°æ¨¡å‹")
        agent = DQNAgent(STATE_DIM, ACTION_DIM)
    
    num_test_episodes = 10
    total_kills = 0
    total_steps = 0
    
    for episode in range(num_test_episodes):
        state = game.reset()
        episode_kills = 0
        episode_steps = 0
        
        print(f"\næµ‹è¯•å›åˆ {episode+1}:")
        
        for step in range(200):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, game)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if action in ACTION_MAP and ACTION_MAP[action]:
                game.do_action(ACTION_MAP[action])
            
            # è‡ªåŠ¨å¼€ç«
            if action in [5, 6]:
                game.player.auto_shoot = True
            
            # æ¸¸æˆæ­¥è¿›
            game.step()
            next_state = game.get_state()
            
            # æ£€æŸ¥å‡»æ€
            kills = game.score // 70 if hasattr(game, 'score') else 0
            if kills > episode_kills:
                episode_kills = kills
                print(f"  æ­¥{step}: å‡»æ€ï¼")
            
            # æ›´æ–°
            state = next_state
            episode_steps += 1
            
            if game.game_over:
                break
            
            # å¤„ç†é€€å‡ºäº‹ä»¶
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    return
        
        total_kills += episode_kills
        total_steps += episode_steps
        
        print(f"  ç»“æŸ: å‡»æ€={episode_kills}, æ­¥æ•°={episode_steps}")
    
    pygame.quit()
    
    avg_kills = total_kills / num_test_episodes
    avg_steps = total_steps / num_test_episodes
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  å¹³å‡æ¯å›åˆå‡»æ€: {avg_kills:.2f}")
    print(f"  å¹³å‡æ¯å›åˆæ­¥æ•°: {avg_steps:.2f}")
    
    if avg_kills > 0.5:
        print("âœ… AIå­¦ä¹ æˆåŠŸï¼")
    else:
        print("âš ï¸  AIä»éœ€æ”¹è¿›")

# ============ ä¸»å‡½æ•° ============
def main():
    print("ğŸ¯ å¦å…‹æ¸¸æˆAI - å·¥ä½œè®­ç»ƒè„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("=" * 60)
    print("åŸºäºè¯Šæ–­ç»“æœè®¾è®¡:")
    print("1. è§„åˆ™AIèƒ½å‡»æ€ â†’ ç¥ç»ç½‘ç»œåº”è¯¥ä¹Ÿèƒ½å­¦ä¹ ")
    print("2. å¢å¼ºå¥–åŠ±å‡½æ•°ï¼Œæ˜ç¡®åé¦ˆ")
    print("3. ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œç»“æ„")
    print("4. ç»éªŒå›æ”¾ + ç›®æ ‡ç½‘ç»œ")
    print("5. ä¿®å¤äº†Pygameå­—ä½“åˆå§‹åŒ–é—®é¢˜")
    print("=" * 60)
    
    while True:
        print("\né€‰é¡¹:")
        print("1. å¼€å§‹è®­ç»ƒDQN")
        print("2. æµ‹è¯•ç°æœ‰æ¨¡å‹")
        print("3. å¿«é€Ÿæµ‹è¯•ï¼ˆåªè¿è¡Œè§„åˆ™AIï¼‰")
        print("4. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        if choice == "1":
            print("\nå¼€å§‹è®­ç»ƒ...")
            agent = train_dqn()
            
            # è®­ç»ƒåç«‹å³æµ‹è¯•
            test_trained_model("./tank_ai_final.pth")
            
        elif choice == "2":
            model_path = input("æ¨¡å‹è·¯å¾„ (é»˜è®¤: ./tank_ai_final.pth): ").strip()
            if not model_path:
                model_path = "./tank_ai_final.pth"
            test_trained_model(model_path)
            
        elif choice == "3":
            print("\nè¿è¡Œè§„åˆ™AIæµ‹è¯•...")
            # ç¡®ä¿Pygameåˆå§‹åŒ–
            try:
                pygame.init()
                pygame.font.init()
            except:
                pass
            
            # ä½¿ç”¨ä¹‹å‰çš„è§„åˆ™AIæµ‹è¯•
            from tankgame import TankGame
            import random
            import math
            
            game = TankGame(render=True)
            
            test_episodes = 5
            total_kills = 0
            
            for ep in range(test_episodes):
                state = game.reset()
                kills = 0
                
                print(f"\nå›åˆ {ep+1}:")
                
                for step in range(200):
                    if game.game_over:
                        break
                    
                    # è§„åˆ™AIé€»è¾‘
                    if game.enemies:
                        enemy = game.enemies[0]
                        dx = enemy.x - game.player.x
                        dy = enemy.y - game.player.y
                        target_angle = math.atan2(-dy, dx)
                        current_angle = game.player.aim_angle
                        
                        angle_diff = (target_angle - current_angle) % (2 * math.pi)
                        if angle_diff > math.pi:
                            angle_diff -= 2 * math.pi
                        
                        if angle_diff > 0.1:
                            action = ACTION_GUN_LEFT
                        elif angle_diff < -0.1:
                            action = ACTION_GUN_RIGHT
                        else:
                            game.player.auto_shoot = True
                            action = random.choice([ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT])
                    else:
                        action = random.choice([ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT])
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    game.do_action(action)
                    game.step()
                    
                    # æ£€æŸ¥å‡»æ€
                    current_kills = game.score // 70
                    if current_kills > kills:
                        kills = current_kills
                        print(f"  æ­¥{step}: å‡»æ€ï¼æ€»å‡»æ€{kills}")
                    
                    # å¤„ç†é€€å‡ºäº‹ä»¶
                    for event in pygame.event.get():
                        if event.type == pygame.QUIT:
                            pygame.quit()
                            return
                
                total_kills += kills
                print(f"  å›åˆç»“æŸ: å‡»æ€{kills}")
            
            pygame.quit()
            
            avg_kills = total_kills / test_episodes
            print(f"\nğŸ“Š è§„åˆ™AIæµ‹è¯•: å¹³å‡æ¯å›åˆ{avg_kills:.1f}å‡»æ€")
            
        elif choice == "4":
            print("ğŸ‘‹ é€€å‡º")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import torch
        main()
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…PyTorch: pip install torch")
        print("è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        
        # ç¡®ä¿Pygameåˆå§‹åŒ–
        try:
            pygame.init()
            pygame.font.init()
        except:
            pass
        
        # è¿è¡Œä¸éœ€è¦PyTorchçš„æµ‹è¯•
        test_episodes = 3
        total_kills = 0
        
        for ep in range(test_episodes):
            game = TankGame(render=True)
            state = game.reset()
            kills = 0
            
            for step in range(200):
                if game.game_over:
                    break
                
                # éšæœºåŠ¨ä½œ
                action = random.choice([ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, 
                                       ACTION_GUN_LEFT, ACTION_GUN_RIGHT])
                game.do_action(action)
                
                # éšæœºå¼€ç«
                if random.random() < 0.3:
                    game.player.auto_shoot = True
                
                game.step()
                
                # æ£€æŸ¥å‡»æ€
                current_kills = game.score // 70
                if current_kills > kills:
                    kills = current_kills
                    print(f"å›åˆ{ep+1} æ­¥{step}: å‡»æ€ï¼")
                
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        pygame.quit()
                        exit()
            
            total_kills += kills
            pygame.quit()
        
        avg_kills = total_kills / test_episodes
        print(f"\néšæœºAIå¹³å‡æ¯å›åˆå‡»æ€: {avg_kills:.1f}")