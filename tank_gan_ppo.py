import os
import sys
# å±è”½macOS Pygameæ— å…³ç³»ç»Ÿè­¦å‘Š
os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = '1'
sys.stdout = open(sys.stdout.fileno(), 'w', buffering=1)
sys.stderr = open(sys.stderr.fileno(), 'w', buffering=1)

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pygame
from tqdm import tqdm
from collections import deque
from tankgame import TankGame

# ====================== AIè¶…å‚æ•°ï¼ˆåŒ¹é…åŸºç¡€ç‰ˆæ¸¸æˆï¼š8ç»´åŠ¨ä½œ+14ç»´çŠ¶æ€ï¼‰ =======================
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
STATE_DIM = 14  # å’Œæ¸¸æˆçš„14ç»´çŠ¶æ€ä¸¥æ ¼ä¸€è‡´
ACTION_DIM = 8  # åŒ¹é…æ¸¸æˆçš„0-7åŠ¨ä½œ
G_LR = 3e-4     # ç”Ÿæˆå™¨å­¦ä¹ ç‡
D_LR = 3e-4     # åˆ¤åˆ«å™¨å­¦ä¹ ç‡
GAMMA = 0.99    # PPOæŠ˜æ‰£å› å­
LAMBDA = 0.95   # PPOä¼˜åŠ¿å‡½æ•°å› å­
EPS_CLIP = 0.2  # PPOè£å‰ªç³»æ•°
BATCH_SIZE = 64 # è®­ç»ƒæ‰¹æ¬¡
UPDATE_EPOCH = 10# PPOæ›´æ–°è½®æ•°
MEMORY_CAPACITY = 100000  # AIç»éªŒæ± å®¹é‡
DEMO_MEMORY_CAPACITY = 10000  # äººç±»æ¼”ç¤ºç»éªŒæ± 
TRAIN_EPISODES = 500  # æ€»è®­ç»ƒå›åˆ
SAVE_INTERVAL = 50    # æ¨¡å‹ä¿å­˜é—´éš”
RENDER_TRAIN = False  # è®­ç»ƒæ—¶å…³é—­æ¸²æŸ“æé€Ÿ
RENDER_TEST = True    # æµ‹è¯•æ—¶å¼€å¯æ¸²æŸ“
TRAIN_STEP_INTERVAL = 2  # æ¯Næ­¥è®­ç»ƒä¸€æ¬¡

# åˆ›å»ºæ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹
if not os.path.exists('./tank_ai_models'):
    os.makedirs('./tank_ai_models')

# ====================== GANç½‘ç»œï¼ˆçº¯æ¨¡å‹ï¼Œè§£è€¦æ¸¸æˆï¼‰ =======================
class Generator(nn.Module):
    """ç”Ÿæˆå™¨ï¼š14ç»´çŠ¶æ€â†’8ç»´åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
    def __init__(self, state_dim, action_dim):
        super(Generator, self).__init__()
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, action_dim)
        self.relu = nn.LeakyReLU(0.2)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return F.softmax(self.fc3(x), dim=-1)

    def get_action(self, state):
        """å¸¦æ¢ç´¢çš„åŠ¨ä½œé€‰æ‹©ï¼ˆå¤šé¡¹å¼é‡‡æ ·ï¼‰"""
        state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            action_probs = self.forward(state)
        action = torch.multinomial(action_probs, 1).item()
        action_prob = action_probs[0, action].item()
        return action, action_prob

    def get_best_action(self, state):
        """æ— æ¢ç´¢çš„æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ï¼ˆè´ªå¿ƒï¼‰"""
        state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            action_probs = self.forward(state)
        return torch.argmax(action_probs, 1).item()

class Discriminator(nn.Module):
    """åˆ¤åˆ«å™¨ï¼š14ç»´çŠ¶æ€+8ç»´åŠ¨ä½œâ†’0-1ä¼˜ç§€åº¦è¯„åˆ†"""
    def __init__(self, state_dim, action_dim):
        super(Discriminator, self).__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 1)
        self.relu = nn.LeakyReLU(0.2)
        self.dropout = nn.Dropout(0.3)
        self.sigmoid = nn.Sigmoid()

    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        return self.sigmoid(self.fc3(x))

# ====================== GAN-PPOæ ¸å¿ƒç®—æ³•ï¼ˆæ ¹æºè§£å†³ç±»å‹é—®é¢˜ï¼‰ =======================
class GAN_PPO:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.G = Generator(state_dim, action_dim).to(DEVICE)
        self.D = Discriminator(state_dim, action_dim).to(DEVICE)
        self.optimizer_G = optim.Adam(self.G.parameters(), lr=G_LR, weight_decay=1e-5)
        self.optimizer_D = optim.Adam(self.D.parameters(), lr=D_LR, weight_decay=1e-5)
        self.memory = deque(maxlen=MEMORY_CAPACITY)
        self.demo_memory = deque(maxlen=DEMO_MEMORY_CAPACITY)
        self.criterion = nn.BCELoss()  # äºŒåˆ†ç±»æŸå¤±

    def one_hot(self, action):
        """åŠ¨ä½œè½¬8ç»´float32ç‹¬çƒ­ç¼–ç """
        action_one_hot = torch.zeros(len(action), self.action_dim, dtype=torch.float32).to(DEVICE)
        action_one_hot[range(len(action)), action] = 1.0
        return action_one_hot

    def store_memory(self, state, action, action_prob, reward, next_state, done):
        """å­˜å‚¨AIè¯•é”™ç»éªŒ"""
        self.memory.append((state, action, action_prob, reward, next_state, done))

    def store_demo_memory(self, state, action, reward):
        """å­˜å‚¨äººç±»æ¼”ç¤ºç»éªŒ"""
        self.demo_memory.append((state, action, reward))

    def train_D(self):
        """è®­ç»ƒåˆ¤åˆ«å™¨ï¼šåŒºåˆ†äººç±»/AIç»éªŒï¼Œæ ¹æºç»Ÿä¸€float32"""
        if len(self.demo_memory) < BATCH_SIZE//2 or len(self.memory) < BATCH_SIZE//2:
            return 0.0
        
        # 1. é‡‡æ ·äººç±»ç»éªŒï¼ˆæ­£æ ·æœ¬ï¼‰- å…¨ç¨‹float32ï¼Œä»æ ¹æºé¿å…double
        demo_idx = np.random.choice(len(self.demo_memory), BATCH_SIZE//2, replace=False)
        demo_data = [self.demo_memory[i] for i in demo_idx]
        demo_s = torch.FloatTensor([d[0] for d in demo_data]).to(DEVICE)
        demo_a = self.one_hot([d[1] for d in demo_data])
        # âœ… æ ¸å¿ƒä¿®å¤ï¼šç”¨torchç”Ÿæˆéšæœºæ•°ï¼Œç›´æ¥æ˜¯float32ï¼Œå½»åº•æŠ›å¼ƒnumpyçš„float64
        demo_l = torch.ones(BATCH_SIZE//2, 1, dtype=torch.float32).to(DEVICE) * torch.FloatTensor(np.random.uniform(0.9, 1.0, (BATCH_SIZE//2, 1))).to(DEVICE)

        # 2. é‡‡æ ·AIç»éªŒï¼ˆè´Ÿæ ·æœ¬ï¼‰- å…¨ç¨‹float32
        ai_idx = np.random.choice(len(self.memory), BATCH_SIZE//2, replace=False)
        ai_data = [self.memory[i] for i in ai_idx]
        ai_s = torch.FloatTensor([d[0] for d in ai_data]).to(DEVICE)
        ai_a = self.one_hot([d[1] for d in ai_data])
        # âœ… æ ¸å¿ƒä¿®å¤ï¼šåŒä¸Šï¼Œtorchç”Ÿæˆfloat32éšæœºæ•°
        ai_l = torch.zeros(BATCH_SIZE//2, 1, dtype=torch.float32).to(DEVICE) * torch.FloatTensor(np.random.uniform(0.0, 0.1, (BATCH_SIZE//2, 1))).to(DEVICE)

        # 3. åˆå¹¶è®­ç»ƒ
        s = torch.cat([demo_s, ai_s], dim=0)
        a = torch.cat([demo_a, ai_a], dim=0)
        labels = torch.cat([demo_l, ai_l], dim=0)

        self.optimizer_D.zero_grad()
        pred = self.D(s, a)  # åˆ¤åˆ«å™¨è¾“å‡ºå¤©ç„¶float32
        loss_D = self.criterion(pred, labels)
        loss_D.backward()
        torch.nn.utils.clip_grad_norm_(self.D.parameters(), max_norm=1.0)
        self.optimizer_D.step()

        return loss_D.item()

    def compute_gae(self, rewards, dones, values, next_values):
        """è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°"""
        gae = 0
        advantages = []
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + GAMMA * next_values[t] * (1 - dones[t]) - values[t]
            gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        advantages = torch.FloatTensor(advantages).to(DEVICE)
        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    def train_G(self):
        """è®­ç»ƒç”Ÿæˆå™¨ï¼šPPOè£å‰ªä¼˜åŒ–"""
        if len(self.memory) < BATCH_SIZE:
            return 0.0
        
        memory = list(self.memory)
        idx = np.random.choice(len(memory), BATCH_SIZE, replace=False)
        batch = [memory[i] for i in idx]

        # ç»Ÿä¸€è½¬æ¢ä¸ºfloat32å¼ é‡
        s = torch.FloatTensor([d[0] for d in batch]).to(DEVICE)
        a = torch.LongTensor([d[1] for d in batch]).to(DEVICE)
        old_p = torch.FloatTensor([d[2] for d in batch]).to(DEVICE).unsqueeze(1)
        r = torch.FloatTensor([d[3] for d in batch]).to(DEVICE).unsqueeze(1)
        ns = torch.FloatTensor([d[4] for d in batch]).to(DEVICE)
        done = torch.FloatTensor([d[5] for d in batch]).to(DEVICE).unsqueeze(1)

        # åˆ¤åˆ«å™¨è®¡ç®—ä»·å€¼
        a_one_hot = self.one_hot(a)
        values = self.D(s, a_one_hot).detach()
        
        # ä¸‹ä¸€çŠ¶æ€ä»·å€¼
        with torch.no_grad():
            next_a_prob = self.G(ns)
            next_a = torch.multinomial(next_a_prob, 1).squeeze()
            next_a_one_hot = self.one_hot(next_a)
            next_values = self.D(ns, next_a_one_hot)

        # GAEä¼˜åŠ¿å‡½æ•°
        advantages = self.compute_gae(
            r.cpu().numpy().squeeze(), 
            done.cpu().numpy().squeeze(),
            values.cpu().numpy().squeeze(), 
            next_values.cpu().numpy().squeeze()
        )

        # PPOè®­ç»ƒ
        loss_G_total = 0.0
        for _ in range(UPDATE_EPOCH):
            new_p = self.G(s).gather(1, a.unsqueeze(1))
            ratio = torch.exp(torch.log(new_p + 1e-8) - torch.log(old_p + 1e-8))
            surr1 = ratio * advantages.unsqueeze(1)
            surr2 = torch.clamp(ratio, 1-EPS_CLIP, 1+EPS_CLIP) * advantages.unsqueeze(1)
            loss_G = -torch.min(surr1, surr2).mean()

            self.optimizer_G.zero_grad()
            loss_G.backward()
            torch.nn.utils.clip_grad_norm_(self.G.parameters(), max_norm=1.0)
            self.optimizer_G.step()
            loss_G_total += loss_G.item()

        return loss_G_total / UPDATE_EPOCH

    def save_model(self, episode):
        """ä¿å­˜ç”Ÿæˆå™¨+åˆ¤åˆ«å™¨æ¨¡å‹"""
        torch.save(self.G.state_dict(), f'./tank_ai_models/generator_ep{episode}.pth')
        torch.save(self.D.state_dict(), f'./tank_ai_models/discriminator_ep{episode}.pth')
        print(f"\næ¨¡å‹ä¿å­˜æˆåŠŸï¼štank_ai_models/ep{episode}")

    def load_model(self, g_path, d_path):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        self.G.load_state_dict(torch.load(g_path, map_location=DEVICE, weights_only=True))
        self.D.load_state_dict(torch.load(d_path, map_location=DEVICE, weights_only=True))
        self.G.eval()
        self.D.eval()
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå·²è¿›å…¥è¯„ä¼°æ¨¡å¼")

# ====================== äººç±»æ¼”ç¤ºæ•°æ®é‡‡é›†ï¼ˆæ— å†—ä½™é€»è¾‘ï¼‰ =======================
def collect_demo_data(game, ppo):
    """é‡‡é›†äººç±»æ“ä½œæ•°æ®ï¼ŒWASDç§»åŠ¨/æ–¹å‘é”®è½¬ç®¡/ç©ºæ ¼å°„å‡»ï¼ŒQé€€å‡º"""
    print("="*50)
    print("å¼€å§‹é‡‡é›†äººç±»æ¼”ç¤ºæ•°æ®ï¼")
    print("æ“ä½œè¯´æ˜ï¼šW(ä¸Š) A(å·¦) S(ä¸‹) D(å³) | â†â†’æ—‹è½¬ç‚®ç®¡ | ç©ºæ ¼å°„å‡» | Qé€€å‡º | ESCé‡ç½®")
    print("="*50)
    state = game.reset()
    demo_count = 0
    clock = pygame.time.Clock()
    demo_data = []
    while True:
        clock.tick(60)
        # äº‹ä»¶å¤„ç†
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()
            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_q:
                    print(f"\né‡‡é›†å®Œæˆï¼å…±æ”¶é›† {demo_count} æ¡äººç±»æ¼”ç¤ºæ•°æ®")
                    return
                if event.key == pygame.K_ESCAPE:
                    state = game.reset()
                    demo_data = []
                    print("æ¸¸æˆå·²é‡ç½®ï¼Œé‡æ–°å¼€å§‹é‡‡é›†")

        # æŒ‰é”®æ˜ å°„
        keys = pygame.key.get_pressed()
        action = 0
        if keys[pygame.K_w]: action = 1
        elif keys[pygame.K_s]: action = 2
        elif keys[pygame.K_a]: action = 3
        elif keys[pygame.K_d]: action = 4
        elif keys[pygame.K_LEFT]: action = 5
        elif keys[pygame.K_RIGHT]: action = 6
        elif keys[pygame.K_SPACE]: action = 7

        # æ‰§è¡ŒåŠ¨ä½œå¹¶å­˜å‚¨æ•°æ®
        game.do_action(action)
        reward, done = game.step()
        next_state = game.get_state()

        # è¿‡æ»¤è¿ç»­æ— æ“ä½œï¼Œåªå­˜å‚¨æœ‰æ•ˆåŠ¨ä½œ
        if not demo_data or not (action == 0 and demo_data[-1] == 0):
            ppo.store_demo_memory(state, action, reward)
            demo_count += 1
            demo_data.append(action)
            if demo_count % 500 == 0:
                print(f"å·²é‡‡é›† {demo_count} æ¡æ¼”ç¤ºæ•°æ® | æœ€æ–°å¥–åŠ±ï¼š{reward:.2f}")

        state = next_state
        if done:
            state = game.reset()
            demo_data = []

# ====================== AIè®­ç»ƒä¸»é€»è¾‘ï¼ˆé‡‡é›†å¼€çª—å£/è®­ç»ƒå…³çª—å£ï¼‰ =======================
def train_ai():
    """å®Œæ•´è®­ç»ƒæµç¨‹ï¼šé‡‡é›†äººç±»æ•°æ® â†’ GAN-PPOè®­ç»ƒ â†’ ä¿å­˜æ¨¡å‹"""
    pygame.init()
    # é‡‡é›†é˜¶æ®µå¼ºåˆ¶å¼€çª—å£ï¼Œä¿è¯èƒ½æ“ä½œ
    game = TankGame(render=True)
    ppo = GAN_PPO(STATE_DIM, ACTION_DIM)

    # é‡‡é›†äººç±»æ¼”ç¤ºæ•°æ®
    collect_demo_data(game, ppo)
    if len(ppo.demo_memory) < 100:
        print("âš ï¸  è­¦å‘Šï¼šæ¼”ç¤ºæ•°æ®ä¸è¶³100æ¡ï¼Œè®­ç»ƒæ•ˆæœå¯èƒ½è¾ƒå·®ï¼")
    
    # é‡‡é›†å®Œæˆï¼Œå…³é—­æ¸²æŸ“æé€Ÿ
    game.render = RENDER_TRAIN

    # å¼€å§‹GAN-PPOè®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹AIè®­ç»ƒï¼å…±{TRAIN_EPISODES}å›åˆ | è®¾å¤‡ï¼š{DEVICE} | æ‰¹æ¬¡ï¼š{BATCH_SIZE}")
    print("="*60)
    clock = pygame.time.Clock()
    for episode in tqdm(range(1, TRAIN_EPISODES+1), desc="AIè®­ç»ƒè¿›åº¦", unit="å›åˆ"):
        state = game.reset()
        total_reward = 0.0
        total_loss_D = 0.0
        total_loss_G = 0.0
        step = 0

        while True:
            step += 1
            clock.tick(100)
            # ç”Ÿæˆå™¨é€‰åŠ¨ä½œ
            action, action_prob = ppo.G.get_action(state)
            game.do_action(action)
            reward, done = game.step()
            next_state = game.get_state()

            # å­˜å‚¨ç»éªŒ
            ppo.store_memory(state, action, action_prob, reward, next_state, done)
            total_reward += reward

            # é—´éš”è®­ç»ƒåˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨
            if step % TRAIN_STEP_INTERVAL == 0:
                loss_D = ppo.train_D()
                loss_G = ppo.train_G()
                total_loss_D += loss_D
                total_loss_G += loss_G

            if done:
                break
            state = next_state

        # æ‰“å°æ¯å›åˆè®­ç»ƒä¿¡æ¯
        avg_loss_D = total_loss_D / step if step > 0 else 0.0
        avg_loss_G = total_loss_G / step if step > 0 else 0.0
        tqdm.write(
            f"å›åˆ{episode:3d} | æ€»å¥–åŠ±{total_reward:6.1f} | DæŸå¤±{avg_loss_D:.4f} | GæŸå¤±{avg_loss_G:.4f} | æ­¥æ•°{step:3d}"
        )

        # æŒ‰é—´éš”ä¿å­˜æ¨¡å‹
        if episode % SAVE_INTERVAL == 0:
            ppo.save_model(episode)

    # è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    ppo.save_model(TRAIN_EPISODES)
    pygame.quit()
    print("\nğŸ‰ AIè®­ç»ƒå®Œæˆï¼æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜è‡³ ./tank_ai_models æ–‡ä»¶å¤¹")

# ====================== AIæµ‹è¯•ä¸»é€»è¾‘ï¼ˆå¯è§†åŒ–è¿è¡Œï¼‰ =======================
def test_ai(g_model_path, d_model_path):
    """æµ‹è¯•è®­ç»ƒå¥½çš„AIï¼Œå¯è§†åŒ–è¿è¡Œ"""
    pygame.init()
    game = TankGame(render=RENDER_TEST)
    ppo = GAN_PPO(STATE_DIM, ACTION_DIM)
    ppo.load_model(g_model_path, d_model_path)

    print("="*50)
    print("å¼€å§‹AIè‡ªåŠ¨ç©æ¸¸æˆï¼æŒ‰Qæˆ–å…³é—­çª—å£é€€å‡º")
    print("="*50)
    clock = pygame.time.Clock()
    while True:
        state = game.reset()
        total_reward = 0.0
        step = 0
        while True:
            clock.tick(60)
            # äº‹ä»¶å¤„ç†
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    return
                if event.type == pygame.KEYDOWN and event.key == pygame.K_q:
                    pygame.quit()
                    return

            # AIé€‰æœ€ä¼˜åŠ¨ä½œæ‰§è¡Œ
            action = ppo.G.get_best_action(state)
            game.do_action(action)
            reward, done = game.step()
            next_state = game.get_state()

            total_reward += reward
            step += 1
            state = next_state

            if done:
                print(f"æµ‹è¯•å›åˆç»“æŸ | æ€»å¥–åŠ±ï¼š{total_reward:.1f} | æ€»æ­¥æ•°ï¼š{step}")
                break

# ====================== è®­ç»ƒ/æµ‹è¯•å…¥å£ =======================
if __name__ == "__main__":
    # ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒAIï¼ˆå…ˆè¿è¡Œè¿™ä¸ªï¼Œé‡‡é›†æ•°æ®å¹¶è®­ç»ƒï¼‰
    train_ai()

    # ç¬¬äºŒæ­¥ï¼šæµ‹è¯•AIï¼ˆè®­ç»ƒå®Œæˆåï¼Œå–æ¶ˆæ³¨é‡Šå¹¶æ›¿æ¢æ¨¡å‹è·¯å¾„ï¼‰
    # g_path = "./tank_ai_models/generator_ep500.pth"
    # d_path = "./tank_ai_models/discriminator_ep500.pth"
    # test_ai(g_path, d_path)