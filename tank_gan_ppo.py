import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pygame
import math
import random
from tqdm import tqdm
from collections import deque

# å¯¼å…¥æ¸¸æˆæ ¸å¿ƒç±»+å¸¸é‡+åŠ¨ä½œï¼ˆä¿®å¤WALL_SIZEæœªå®šä¹‰ï¼‰
from tankgame import (
    TankGame, ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT,
    ACTION_GUN_LEFT, ACTION_GUN_RIGHT, WALL_SIZE
)

# ====================== åŸºç¡€é…ç½® =======================
os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = '1'
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"âœ… è®­ç»ƒè®¾å¤‡ï¼š{DEVICE} | CUDAå¯ç”¨ï¼š{torch.cuda.is_available()}")

# æ ¸å¿ƒç»´åº¦ï¼ˆAIä»…è¾“å‡º0/1åŠ¨ä½œç±»å‹ï¼Œæ˜ å°„æ¸¸æˆå®é™…åŠ¨ä½œï¼‰
STATE_DIM = 14
ACTION_DIM = 2  # 0=ç§»åŠ¨ 1=ç„å‡†
DEMO_VEC_DIM = STATE_DIM + ACTION_DIM

# ====================== è¶…å‚æ•°ï¼ˆå¾®è°ƒGANæƒé‡è‡³0.08ï¼Œæ›´ä¿å®ˆï¼‰ ======================
# PPOè¶…å‚æ•°
PPO_LR = 3e-4
GAMMA = 0.98
LAMBDA = 0.85
EPS_CLIP = 0.25
BATCH_SIZE = 32
ENT_COEF = 0.1
MAX_STEP = 350
# GANè¶…å‚æ•°ï¼ˆğŸ”§ æŒ‰è¦æ±‚è®¾ä¸º0.05~0.1åŒºé—´ï¼Œé€‰0.08æ›´ç¨³å¦¥ï¼‰
GAN_LR = 4e-5
GAN_UPDATE_INTERVAL = 5
GAN_REWARD_WEIGHT = 0.08  # æ ¸å¿ƒï¼šå°æƒé‡èåˆï¼Œä¸ä¸»å¯¼å¥–åŠ±

# ====================== å¥–åŠ±/æƒ©ç½šé…ç½®ï¼ˆæ–°å¢ç„å‡†ç¡¬å¥–åŠ±5.0ï¼‰ ======================
REWARD_DIRECT_SHOT_POS = 3.0     # ç§»åŠ¨ç›´å°„ä½å¥–åŠ±
REWARD_DODGE_BULLET = 1.5       # èº²å­å¼¹å¥–åŠ±
REWARD_AIM_EXPOSED = 2.0        # åŸç„å‡†å¥–åŠ±
REWARD_AIM_PERFECT = 5.0        # ğŸ”§ æ–°å¢ï¼šå®Œç¾ç„å‡†ç¡¬å¥–åŠ±ï¼ˆæ›´é«˜ï¼‰
PUNISH_NO_KILL = 1.5            # æœªå‡»æ€æƒ©ç½š
PUNISH_IDLE = 1.0               # é‡å¤åŠ¨ä½œæƒ©ç½š
PUNISH_BEEN_HIT = 30.0          # è¢«å‡»ä¸­æƒ©ç½š
# åˆ¤å®šé˜ˆå€¼
BULLET_DODGE_DIST = 50          # å­å¼¹å®‰å…¨è·ç¦»
NO_KILL_STEP_THRESH = 20        # æœªå‡»æ€æƒ©ç½šæ­¥æ•°
CLOSE_DIST_THRESH = 200         # æ•Œäººæš´éœ²è·ç¦»
RAYCAST_STEP = 10               # å°„çº¿æ£€æµ‹æ­¥é•¿
AIM_PERFECT_THRESH = 0.1        # ğŸ”§ æ–°å¢ï¼šå®Œç¾ç„å‡†è¯¯å·®é˜ˆå€¼

# ====================== è®­ç»ƒé…ç½® ======================
MEMORY_CAPACITY = 60000
DEMO_MEMORY_CAPACITY = 4000
TRAIN_EPISODES = 1200
SAVE_INTERVAL = 100
RENDER_TRAIN = False
RENDER_TEST = True

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs("./tank_ai_models_simple", exist_ok=True)
os.makedirs("./tank_demo_data_simple", exist_ok=True)

# =========================================================
# PPO ç»éªŒç¼“å†²åŒº
# =========================================================
class PPOMemory:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = deque(maxlen=capacity)
    
    def add(self, s, a, r, ns, d, p):
        self.memory.append((s, a, r, ns, d, p))
    
    def sample(self, batch_size):
        if len(self.memory) < batch_size:
            return None
        idx = np.random.choice(len(self.memory), batch_size, replace=False)
        data = [self.memory[i] for i in idx]
        s = torch.FloatTensor([d[0] for d in data]).to(DEVICE)
        a = torch.LongTensor([d[1] for d in data]).unsqueeze(1).to(DEVICE)
        r = torch.FloatTensor([d[2] for d in data]).unsqueeze(1).to(DEVICE)
        ns = torch.FloatTensor([d[3] for d in data]).to(DEVICE)
        d = torch.FloatTensor([d[4] for d in data]).unsqueeze(1).to(DEVICE)
        p = torch.FloatTensor([d[5] for d in data]).unsqueeze(1).to(DEVICE)
        return s, a, r, ns, d, p
    
    def __len__(self):
        return len(self.memory)

# =========================================================
# ä¸“å®¶ç»éªŒç¼“å†²åŒº
# =========================================================
class DemoMemory:
    def __init__(self, capacity, vec_dim):
        self.capacity = capacity
        self.vec_dim = vec_dim
        self.memory = deque(maxlen=capacity)
    
    def add(self, demo_vec):
        assert demo_vec.shape[0] == self.vec_dim, f"ç»´åº¦é”™è¯¯ï¼šå®é™…{demo_vec.shape[0]}ï¼ŒæœŸæœ›{self.vec_dim}"
        self.memory.append(demo_vec)
    
    def sample(self, batch_size):
        batch_size = min(batch_size, len(self.memory))
        idx = np.random.choice(len(self.memory), batch_size, replace=False)
        demo_array = np.array([self.memory[i] for i in idx], dtype=np.float32)
        return torch.FloatTensor(demo_array).to(DEVICE)
    
    def save_to_npy(self, npy_path):
        np.save(npy_path, np.array(self.memory, dtype=np.float32))
        print(f"ğŸ’¾ ä¸“å®¶ç»éªŒä¿å­˜è‡³ï¼š{npy_path}")
    
    def load_from_npy(self, npy_path):
        if os.path.exists(npy_path):
            demo_array = np.load(npy_path)
            for vec in demo_array:
                self.add(vec)
            print(f"ğŸ“š åŠ è½½ä¸“å®¶ç»éªŒï¼š{len(demo_array)}æ¡ | ç»´åº¦ï¼š{demo_array.shape[1]}")
    
    def __len__(self):
        return len(self.memory)

# =========================================================
# PPO ç½‘ç»œï¼ˆğŸ”§ æ”¹åŠ¨1ï¼šget_best_actionåŠ ç„å‡†æ–¹å‘åå¥½ï¼Œ7:3æ¯”ä¾‹ï¼‰
# =========================================================
class PPO_Actor(nn.Module):
    def __init__(self, state_dim, action_dim=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.LeakyReLU(0.1),
            nn.Linear(64, 32),
            nn.LeakyReLU(0.1),
            nn.Linear(32, action_dim)
        ).to(DEVICE)
        # AIåŠ¨ä½œç±»å‹å¯¹åº”çš„æ¸¸æˆåŠ¨ä½œåˆ—è¡¨
        self.MOVE_ACTION_LIST = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]
        self.AIM_ACTION_LIST = [ACTION_GUN_LEFT, ACTION_GUN_RIGHT]

    def forward(self, x):
        return F.softmax(self.net(x.to(DEVICE)), dim=-1)

    def get_action(self, state):
        """è®­ç»ƒç”¨ï¼šè¿”å›æ¸¸æˆåŠ¨ä½œã€AI0/1åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡"""
        s = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            probs = self(s)
        ai_action = torch.multinomial(probs, 1).item()
        # æŒ‰AIç±»å‹éšæœºé€‰æ¸¸æˆåŠ¨ä½œ
        if ai_action == 0:
            game_action = random.choice(self.MOVE_ACTION_LIST)
        else:
            game_action = random.choice(self.AIM_ACTION_LIST)
        return game_action, ai_action, probs[0, ai_action].item()

    def get_best_action(self, state):
        """ğŸ”§ æ”¹åŠ¨1ï¼šç„å‡†åŠ¨ä½œ7:3æ–¹å‘åå¥½ï¼Œç‚®ç®¡è¿ç»­è½¬ä¸æŠ–åŠ¨ï¼ˆä»…æµ‹è¯•ç”¨ï¼Œé›¶é£é™©ï¼‰"""
        s = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            probs = self(s)

        ai_action = torch.argmax(probs, dim=1).item()

        if ai_action == 0:
            return random.choice(self.MOVE_ACTION_LIST)
        else:
            # 70%å·¦ç„å‡†ï¼Œ30%å³ç„å‡†ï¼ŒåŠ¨ä½œä¸€è‡´ä¸æŠ–åŠ¨
            return ACTION_GUN_LEFT if random.random() < 0.7 else ACTION_GUN_RIGHT

class PPO_Critic(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.LeakyReLU(0.1),
            nn.Linear(64, 32),
            nn.LeakyReLU(0.1),
            nn.Linear(32, 1)
        ).to(DEVICE)

    def forward(self, x):
        return self.net(x.to(DEVICE))

# =========================================================
# GAN åˆ¤åˆ«å™¨ï¼ˆæ— æ”¹åŠ¨ï¼Œä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
# =========================================================
class GAILGAN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.discriminator = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.LeakyReLU(0.1),
            nn.Linear(128, 1)
        ).to(DEVICE)
        self.optimizer = optim.Adam(self.discriminator.parameters(), lr=GAN_LR)
    
    def forward(self, x):
        return self.discriminator(x.to(DEVICE))
    
    def train_step(self, agent_batch, expert_batch):
        expert_logits = self(expert_batch)
        agent_logits = self(agent_batch)
        loss_expert = F.binary_cross_entropy_with_logits(expert_logits, torch.ones_like(expert_logits).to(DEVICE))
        loss_agent = F.binary_cross_entropy_with_logits(agent_logits, torch.zeros_like(agent_logits).to(DEVICE))
        loss = 0.5 * (loss_expert + loss_agent)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        with torch.no_grad():
            gan_reward = -torch.log(torch.sigmoid(agent_logits) + 1e-8).mean().item()
        return loss.item(), gan_reward

# =========================================================
# PPO-GAN æ™ºèƒ½ä½“ï¼ˆæ— æ”¹åŠ¨ï¼‰
# =========================================================
class PPO_GAN_Simple:
    def __init__(self):
        self.actor = PPO_Actor(STATE_DIM, ACTION_DIM)
        self.critic = PPO_Critic(STATE_DIM)
        self.ppo_opt = optim.Adam(list(self.actor.parameters()) + list(self.critic.parameters()), lr=PPO_LR)
        self.ppo_memory = PPOMemory(MEMORY_CAPACITY)
        self.demo_memory = DemoMemory(DEMO_MEMORY_CAPACITY, DEMO_VEC_DIM)
        self.gan = GAILGAN(DEMO_VEC_DIM)
        self.reset_combat_state()

    def reset_combat_state(self):
        self.no_kill_step = 0
        self.last_action = -1
        self.last_player_pos = (0, 0)

    def one_hot(self, actions):
        oh = torch.zeros(len(actions), ACTION_DIM).to(DEVICE)
        oh[range(len(actions)), actions] = 1.0
        return oh
    
    def compute_gae(self, r, d, v, nv):
        gae = 0
        adv = torch.zeros_like(r).to(DEVICE)
        for i in reversed(range(len(r))):
            delta = r[i] + GAMMA * nv[i] * (1 - d[i]) - v[i]
            gae = delta + GAMMA * LAMBDA * (1 - d[i]) * gae
            adv[i] = gae
        return adv.clamp(-1.0, 1.0)

    def train_ppo(self):
        """PPOè®­ç»ƒï¼šç´¢å¼•å·²åŒ¹é…ï¼ˆaæ˜¯0/1ï¼‰ï¼Œæ— è¶Šç•Œ"""
        batch = self.ppo_memory.sample(BATCH_SIZE)
        if batch is None:
            return 0.0
        s, a, r, ns, d, old_p = batch
        with torch.no_grad():
            v = self.critic(s)
            nv = self.critic(ns)
        adv = self.compute_gae(r, d, v, nv)
        target_v = adv + v
        probs = self.actor(s)
        new_p = probs.gather(1, a)
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1).mean()
        ratio = torch.exp(torch.log(new_p + 1e-8) - torch.log(old_p + 1e-8))
        surr1 = ratio * adv
        surr2 = torch.clamp(ratio, 1 - EPS_CLIP, 1 + EPS_CLIP) * adv
        actor_loss = -torch.min(surr1, surr2).mean()
        critic_loss = F.mse_loss(self.critic(s), target_v)
        total_loss = actor_loss + 0.5 * critic_loss - ENT_COEF * entropy
        self.ppo_opt.zero_grad()
        total_loss.backward()
        self.ppo_opt.step()
        return total_loss.item()

    def save(self, ep):
        save_path = f"./tank_ai_models_simple/ppo_gan_simple_ep{ep}.pth"
        torch.save({"actor": self.actor.state_dict(), "critic": self.critic.state_dict()}, save_path)
        print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜ï¼š{save_path}")

    def load(self, model_path):
        checkpoint = torch.load(model_path, map_location=DEVICE)
        self.actor.load_state_dict(checkpoint["actor"])
        self.critic.load_state_dict(checkpoint["critic"])
        print(f"âœ… åŠ è½½æ¨¡å‹ï¼š{model_path}")

# =========================================================
# å·¥å…·å‡½æ•°ï¼ˆæ— æ”¹åŠ¨ï¼Œå°„çº¿æ£€æµ‹æ­£å¸¸ï¼‰
# =========================================================
def get_nearest_enemy(game):
    enemies_alive = [e for e in game.enemies if e.alive]
    if not enemies_alive:
        return None
    distances = [math.hypot(e.x - game.player.x, e.y - game.player.y) for e in enemies_alive]
    return enemies_alive[np.argmin(distances)]

def raycast_obstacle(game, start_pos, end_pos):
    sx, sy = start_pos
    ex, ey = end_pos
    dx = ex - sx
    dy = ey - sy
    dist = math.hypot(dx, dy)
    if dist == 0:
        return True
    step_x = (dx / dist) * RAYCAST_STEP
    step_y = (dy / dist) * RAYCAST_STEP
    current_x, current_y = sx, sy
    for _ in range(int(dist // RAYCAST_STEP) + 1):
        current_x += step_x
        current_y += step_y
        for wall in game.walls:
            wall_rect = pygame.Rect(wall[0], wall[1], WALL_SIZE, WALL_SIZE)
            if wall_rect.collidepoint(current_x, current_y):
                return True
    return False

def is_in_direct_shot_position(game):
    enemy = get_nearest_enemy(game)
    if not enemy or not game.player.alive:
        return False
    return not raycast_obstacle(game, (game.player.x, game.player.y), (enemy.x, enemy.y))

def is_dodging_bullet(game):
    if not hasattr(game, "bullets") or len(game.bullets) == 0:
        return True
    player_x, player_y = game.player.x, game.player.y
    for bullet in game.bullets:
        if not bullet.is_player_bullet and math.hypot(bullet.x - player_x, bullet.y - player_y) <= BULLET_DODGE_DIST:
            return False
    return True

def is_enemy_exposed(game):
    enemy = get_nearest_enemy(game)
    if not enemy:
        return False
    return math.hypot(enemy.x - game.player.x, enemy.y - game.player.y) < CLOSE_DIST_THRESH

def calculate_aim_error(game):
    enemy = get_nearest_enemy(game)
    if not enemy:
        return 1.0
    dx = enemy.x - game.player.x
    dy = enemy.y - game.player.y
    target_angle = math.atan2(-dy, dx) % (2 * math.pi)
    current_angle = game.player.aim_angle % (2 * math.pi)
    angle_error = abs(current_angle - target_angle)
    angle_error = min(angle_error, 2 * math.pi - angle_error)
    return angle_error / math.pi

# =========================================================
# å¥–åŠ±å‡½æ•°ï¼ˆğŸ”§ æ”¹åŠ¨2ï¼šæ–°å¢å®Œç¾ç„å‡†ç¡¬å¥–åŠ±5.0ï¼Œä¸åˆ åŸé€»è¾‘ï¼‰
# =========================================================
def get_env_reward(game, action, agent):
    final_reward = 0.0
    enemy_visible = get_nearest_enemy(game) is not None and game.player.alive

    # ç§»åŠ¨åŠ¨ä½œå¥–åŠ±
    if action in [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT] and enemy_visible:
        if is_in_direct_shot_position(game):
            final_reward += REWARD_DIRECT_SHOT_POS
        if is_dodging_bullet(game):
            final_reward += REWARD_DODGE_BULLET
    # åŸç„å‡†åŠ¨ä½œå¥–åŠ±
    if action in [ACTION_GUN_LEFT, ACTION_GUN_RIGHT] and enemy_visible and is_enemy_exposed(game):
        aim_error = calculate_aim_error(game)
        if aim_error < 0.2:
            final_reward += REWARD_AIM_EXPOSED * (1 - aim_error)
    
    # ğŸ”§ æ”¹åŠ¨2ï¼šæ–°å¢å®Œç¾ç„å‡†ç¡¬å¥–åŠ±ï¼ˆè¯¯å·®<10%åŠ 5.0ï¼Œè¿œé«˜äºç§»åŠ¨å¥–åŠ±ï¼‰
    if action in [ACTION_GUN_LEFT, ACTION_GUN_RIGHT] and enemy_visible:
        aim_error = calculate_aim_error(game)
        if aim_error < AIM_PERFECT_THRESH:
            final_reward += REWARD_AIM_PERFECT

    # å„é¡¹æƒ©ç½š
    if enemy_visible:
        agent.no_kill_step += 1
        if agent.no_kill_step >= NO_KILL_STEP_THRESH:
            final_reward -= PUNISH_NO_KILL
            agent.no_kill_step = NO_KILL_STEP_THRESH - 10
    if agent.last_action == action and agent.last_action != -1:
        final_reward -= PUNISH_IDLE
    if hasattr(game.player, 'been_hit') and game.player.been_hit:
        final_reward -= PUNISH_BEEN_HIT
        game.player.been_hit = False

    game.player.auto_shoot = True
    agent.last_action = action
    agent.last_player_pos = (game.player.x, game.player.y)
    return np.clip(final_reward, -50, 50)

# =========================================================
# ç”Ÿæˆä¸“å®¶ç»éªŒï¼ˆæ— æ”¹åŠ¨ï¼‰
# =========================================================
def generate_demo_data(demo_memory, demo_num=1000):
    print("\nğŸ® ç”Ÿæˆä¸“å®¶ç»éªŒ | æŒ‰é”®ï¼šWASD=ç§»åŠ¨ | â†â†’=ç„å‡† | ESCé€€å‡º")
    print(f"ğŸ¯ ç›®æ ‡é‡‡é›†ï¼š{demo_num}æ¡æœ‰æ•ˆç›´å°„ä½ç»éªŒ")
    game = TankGame(render=True)
    state = game.reset()
    clock = pygame.time.Clock()
    running = True

    while running and len(demo_memory) < demo_num:
        clock.tick(60)
        action = 0
        for event in pygame.event.get():
            if event.type == pygame.QUIT or (event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE):
                running = False
                break
        if not running:
            break
        # æ‰‹åŠ¨æ“ä½œæ˜ å°„AIåŠ¨ä½œç±»å‹
        keys = pygame.key.get_pressed()
        ai_action_type = 0
        if keys[pygame.K_w] or keys[pygame.K_s] or keys[pygame.K_a] or keys[pygame.K_d]:
            ai_action_type = 0
            action = ACTION_UP if keys[pygame.K_w] else ACTION_DOWN if keys[pygame.K_s] else ACTION_LEFT if keys[pygame.K_a] else ACTION_RIGHT
        elif keys[pygame.K_LEFT] or keys[pygame.K_RIGHT]:
            ai_action_type = 1
            action = ACTION_GUN_LEFT if keys[pygame.K_LEFT] else ACTION_GUN_RIGHT
        # ç›´å°„ä½é‡‡é›†ç»éªŒ
        if game.player.alive and is_in_direct_shot_position(game):
            state_np = np.asarray(state, dtype=np.float32).flatten()
            action_one_hot = np.eye(ACTION_DIM, dtype=np.float32)[ai_action_type]
            demo_vec = np.concatenate([state_np, action_one_hot])
            demo_memory.add(demo_vec)
            if len(demo_memory) % 100 == 0:
                print(f"ğŸ“ˆ å·²é‡‡é›†ï¼š{len(demo_memory)}/{demo_num} æ¡")
        # æ‰§è¡ŒåŠ¨ä½œ
        game.do_action(action)
        game.player.auto_shoot = True
        game.step()
        state = game.get_state()
        if game.game_over:
            state = game.reset()
            print(f"ğŸ”„ æ¸¸æˆé‡ç½® | ç»§ç»­é‡‡é›†ç»éªŒ...")
    # ä¿å­˜ç»éªŒ
    demo_memory.save_to_npy("./tank_demo_data_simple/demo_memory_simple.npy")
    pygame.quit()
    print(f"\nâœ… ä¸“å®¶ç»éªŒç”Ÿæˆå®Œæˆï¼å®é™…é‡‡é›†ï¼š{len(demo_memory)}æ¡")

# =========================================================
# è®­ç»ƒå…¥å£ï¼ˆğŸ”§ æ”¹åŠ¨3ï¼šèåˆGANå¥–åŠ±åˆ°ç¯å¢ƒå¥–åŠ±ï¼ŒGANä¸å†ç™½è®­ç»ƒï¼‰
# =========================================================
def train_ai(load_model_path=None):
    pygame.init()
    game = TankGame(render=RENDER_TRAIN)
    agent = PPO_GAN_Simple()

    # åŠ è½½/é‡‡é›†ä¸“å®¶ç»éªŒ
    demo_path = "./tank_demo_data_simple/demo_memory_simple.npy"
    if os.path.exists(demo_path):
        agent.demo_memory.load_from_npy(demo_path)
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°ä¸“å®¶ç»éªŒï¼Œå¼€å§‹æ‰‹åŠ¨é‡‡é›†...")
        generate_demo_data(agent.demo_memory, 1000)
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if load_model_path and os.path.exists(load_model_path):
        agent.load(load_model_path)
    # ç»´åº¦æ ¡éªŒ
    assert len(game.get_state()) == STATE_DIM, f"çŠ¶æ€ç»´åº¦ä¸åŒ¹é…ï¼æ¸¸æˆ{len(game.get_state())} vs é…ç½®{STATE_DIM}"
    print(f"\nğŸš€ æ­£å¼å¼€å§‹è®­ç»ƒ | æ€»è½®æ•°ï¼š{TRAIN_EPISODES} | GANå¥–åŠ±æƒé‡ï¼š{GAN_REWARD_WEIGHT}")
    print(f"ğŸ“Œ æ ¸å¿ƒå¥–åŠ±ï¼šå®Œç¾ç„å‡†+{REWARD_AIM_PERFECT} | ç›´å°„ä½+{REWARD_DIRECT_SHOT_POS}")
    pbar = tqdm(range(1, TRAIN_EPISODES + 1), desc="è®­ç»ƒè¿›åº¦")

    for ep in pbar:
        state = game.reset()
        agent.reset_combat_state()
        total_reward = 0.0
        step = 0
        ppo_loss_sum = 0.0
        ppo_train_count = 0
        gan_reward_sum = 0.0  # æ–°å¢ï¼šç»Ÿè®¡GANå¥–åŠ±å‡å€¼

        while step < MAX_STEP and not game.game_over:
            step += 1
            # è·å–åŠ¨ä½œ
            game_action, ai_action, action_prob = agent.actor.get_action(state)
            game.do_action(game_action)
            game.player.auto_shoot = True
            _, done = game.step()
            # è®¡ç®—åŸºç¡€ç¯å¢ƒå¥–åŠ±
            base_reward = get_env_reward(game, game_action, agent)
            next_state = game.get_state()

            # ğŸ”§ æ”¹åŠ¨3ï¼šè®­ç»ƒGANå¹¶èåˆå¥–åŠ±ï¼ˆå°æƒé‡ï¼Œä¸ä¸»å¯¼ï¼‰
            gan_reward = 0.0
            if step % GAN_UPDATE_INTERVAL == 0 and len(agent.ppo_memory) >= BATCH_SIZE:
                batch = agent.ppo_memory.sample(BATCH_SIZE)
                if batch is not None:
                    s_ppo, a_ppo, _, _, _, _ = batch
                    ai_action_onehot = agent.one_hot(a_ppo.squeeze(1))
                    agent_batch = torch.cat([s_ppo, ai_action_onehot], dim=-1)
                    expert_batch = agent.demo_memory.sample(BATCH_SIZE)
                    # è®­ç»ƒGANå¹¶è·å–GANå¥–åŠ±
                    gan_loss, gan_reward = agent.gan.train_step(agent_batch, expert_batch)
                    gan_reward_sum += gan_reward
            # èåˆæ€»å¥–åŠ±ï¼šç¯å¢ƒå¥–åŠ± + å°æƒé‡GANå¥–åŠ±
            total_reward_step = base_reward + GAN_REWARD_WEIGHT * gan_reward

            # å­˜å‚¨ç»éªŒï¼ˆç”¨èåˆåçš„æ€»å¥–åŠ±ï¼‰
            agent.ppo_memory.add(state, ai_action, total_reward_step, next_state, done, action_prob)
            total_reward += total_reward_step

            # è®­ç»ƒPPO
            if step % 3 == 0:
                ppo_loss = agent.train_ppo()
                ppo_loss_sum += ppo_loss
                ppo_train_count += 1
            
            # æ›´æ–°çŠ¶æ€
            state = next_state

        # è¿›åº¦æ¡å±•ç¤ºï¼šæ–°å¢GANå¥–åŠ±å‡å€¼ï¼Œç›´è§‚çœ‹åˆ°GANæ•ˆæœ
        avg_ppo_loss = ppo_loss_sum / max(ppo_train_count, 1)
        avg_gan_reward = gan_reward_sum / max(step // GAN_UPDATE_INTERVAL, 1)
        pbar.set_postfix({
            "æ€»å¥–åŠ±": f"{total_reward:.2f}",
            "PPOæŸå¤±": f"{avg_ppo_loss:.4f}",
            "å¹³å‡GANå¥–åŠ±": f"{avg_gan_reward:.3f}",
            "ç»éªŒæ± ": f"{len(agent.ppo_memory)}/{MEMORY_CAPACITY}"
        })
        # ä¿å­˜æ¨¡å‹
        if ep % SAVE_INTERVAL == 0:
            agent.save(ep)

    # è®­ç»ƒå®Œæˆ
    agent.save(TRAIN_EPISODES)
    pygame.quit()
    print(f"\nğŸ‰ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³ï¼š./tank_ai_models_simple/ppo_gan_simple_ep{TRAIN_EPISODES}.pth")

# =========================================================
# æµ‹è¯•å…¥å£ï¼ˆè°ƒç”¨æ”¹åŠ¨åçš„get_best_actionï¼Œç„å‡†æœ‰æ–¹å‘åå¥½ï¼‰
# =========================================================
def test_ai(model_path):
    pygame.init()
    game = TankGame(render=RENDER_TEST)
    agent = PPO_GAN_Simple()
    agent.load(model_path)

    state = game.reset()
    agent.reset_combat_state()
    clock = pygame.time.Clock()
    total_reward = 0.0
    step = 0
    kill_num = 0
    print(f"\nğŸ® AIæµ‹è¯•å¯åŠ¨ | æ¨¡å‹ï¼š{model_path} | æœ€å¤§æ­¥æ•°ï¼š{MAX_STEP}")
    print(f"ğŸ’¡ æŒ‰ Q æˆ– å…³é—­çª—å£ é€€å‡ºæµ‹è¯• | ç„å‡†åå¥½ï¼š70%å·¦ | 30%å³")

    while step < MAX_STEP and not game.game_over:
        step += 1
        clock.tick(60)
        # è°ƒç”¨æœ‰æ–¹å‘åå¥½çš„get_best_action
        action = agent.actor.get_best_action(state)
        game.do_action(action)
        game.player.auto_shoot = True
        game.step()
        reward = get_env_reward(game, action, agent)
        total_reward += reward
        state = game.get_state()
        kill_num = game.score // 70 if game.score > 0 else 0

        # é€€å‡ºç›‘å¬
        for event in pygame.event.get():
            if event.type == pygame.QUIT or (event.type == pygame.KEYDOWN and event.key == pygame.K_q):
                pygame.quit()
                print(f"\nğŸ›‘ æµ‹è¯•æ‰‹åŠ¨é€€å‡º")
                return

    # æµ‹è¯•ç»“æœ
    pygame.quit()
    print(f"\nâœ… æµ‹è¯•ç»“æŸ | æœ€ç»ˆç»Ÿè®¡ï¼š")
    print(f"ğŸ“Š æ­¥æ•°ï¼š{step} | æ€»å¥–åŠ±ï¼š{total_reward:.2f} | æ€»å¾—åˆ†ï¼š{game.score}")
    print(f"ğŸ† å‡»æ€æ•°ï¼š{kill_num} | å‰©ä½™ç”Ÿå‘½ï¼š{game.player.lives}")
    print(f"ğŸ“ˆ æµ‹è¯•ç»“æœï¼š{'èƒœåˆ©' if kill_num >=8 else 'å¤±è´¥'}ï¼ˆèƒœåˆ©æ¡ä»¶ï¼šå‡»æ€â‰¥8ï¼‰")

# =========================================================
# ä¸»å‡½æ•°ï¼ˆè®­ç»ƒ/æµ‹è¯•ä¸€é”®åˆ‡æ¢ï¼‰
# =========================================================
if __name__ == "__main__":
    TRAIN_MODE = True  # True=è®­ç»ƒï¼ŒFalse=æµ‹è¯•
    PRETRAIN_MODEL = None  # ç»§ç»­è®­ç»ƒçš„æ¨¡å‹è·¯å¾„ï¼ŒNoneåˆ™ä»å¤´è®­ç»ƒ
    TEST_MODEL = "./tank_ai_models_simple/ppo_gan_simple_ep1200.pth"  # æµ‹è¯•æ¨¡å‹è·¯å¾„

    if TRAIN_MODE:
        train_ai(load_model_path=PRETRAIN_MODEL)
    else:
        if not os.path.exists(TEST_MODEL):
            print(f"âŒ æµ‹è¯•æ¨¡å‹ä¸å­˜åœ¨ï¼š{TEST_MODEL}")
            print(f"ğŸ’¡ è¯·å…ˆæ‰§è¡Œè®­ç»ƒæ¨¡å¼ç”Ÿæˆæ¨¡å‹")
        else:
            test_ai(model_path=TEST_MODEL)